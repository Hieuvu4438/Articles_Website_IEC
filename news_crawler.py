import json
import time
import requests
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import schedule
import os
from datetime import datetime

class NewsCrawler:
    def __init__(self):
        """Kh·ªüi t·∫°o News Crawler"""
        self.GEMINI_API_KEY = "AIzaSyCprlPj5kLhDb6hU-Z39qh4VZTcc_7gMAM"
        self.load_configs()
        
    def load_configs(self):
        """Load c·∫•u h√¨nh parser t·ª´ file JSON"""
        with open("parsers.json", "r", encoding="utf-8") as f:
            self.PARSERS = json.load(f)

        with open("parsers_links.json", "r", encoding="utf-8") as f:
            self.PARSERS_LINKS = json.load(f)
            
        # Load topics categories
        try:
            with open("topics_categories.json", "r", encoding="utf-8") as f:
                self.TOPICS_CATEGORIES = json.load(f)
        except FileNotFoundError:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y topics_categories.json")
            self.TOPICS_CATEGORIES = {"categories": {}}

    def get_parser_by_domain(self, domain):
        """L·∫•y c·∫•u h√¨nh parser theo domain"""
        for config in self.PARSERS.values():
            if config["domain"] in domain:
                return config
        return None

    def get_parser_link(self, domain):
        """L·∫•y c·∫•u h√¨nh parser links theo domain"""
        for config in self.PARSERS_LINKS.values():
            if config["domain"] in domain:
                return config
        return None

    def get_latest_news(self, soup, config):
        """L·∫•y danh s√°ch links tin t·ª©c m·ªõi nh·∫•t"""
        links_tags = soup.select(config["links"])
        links = [a['href'] for a in links_tags]
        return links

    def extract_by_config(self, soup, config):
        """Tr√≠ch xu·∫•t d·ªØ li·ªáu b√†i vi·∫øt theo c·∫•u h√¨nh"""
        result = {
            "title": "",
            "author": "",
            "time": "",
            "topics": [],
            "content": []
        }
        
        title_tag = soup.select_one(config.get("title", ""))
        if title_tag:
            result["title"] = title_tag.get_text(strip=True)
            
        author_tag = soup.select_one(config.get("author", ""))
        if author_tag:
            result["author"] = author_tag.get_text(strip=True)
            
        time_tag = soup.select_one(config.get("time", ""))
        if time_tag:
            result["time"] = time_tag.get_text(strip=True)
            
        topic_tags = soup.select(config.get("topic", ""))
        if topic_tags:
            result["topics"] = [tag.get_text(strip=True) for tag in topic_tags]
            
        content_container = soup.select_one(config.get("content", ""))
        if content_container:
            all_elements = content_container.find_all(recursive=True)
            for element in all_elements:
                if element.name == "p" and element.get_text(strip=True):
                    result["content"].append({"type": "text", "value": element.get_text(strip=True)})
                elif element.name == "img" and element.get("src", ""):
                    result["content"].append({"type": "image", "value": element.get("src")})
                elif element.name == "a" and element.get("href", ""):
                    result["content"].append({"type": "link", "value": element.get("href")})
                    
        return result

    def summarize_with_gemini(self, content):
        """T√≥m t·∫Øt n·ªôi dung s·ª≠ d·ª•ng Gemini AI"""
        prompt = (
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t√≥m t·∫Øt tin t·ª©c v·ªõi ƒë·ªô ch√≠nh x√°c cao. "
            "Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc ƒëo·∫°n n·ªôi dung sau v√† t√≥m t·∫Øt c√°c √Ω ch√≠nh m·ªôt c√°ch s√∫c t√≠ch, d·ªÖ hi·ªÉu, gi·ªØ nguy√™n tinh th·∫ßn v√† th√¥ng tin quan tr·ªçng c·ªßa b√†i vi·∫øt. "
            "T√≥m t·∫Øt ng·∫Øn g·ªçn, s·ª≠ d·ª•ng ng√¥n ng·ªØ t·ª± nhi√™n, trung l·∫≠p v√† kh√¥ng th√™m th√¥ng tin ngo√†i n·ªôi dung g·ªëc. "
            "Tr√¨nh b√†y t√≥m t·∫Øt theo ƒëo·∫°n vƒÉn b·∫£n, kh√¥ng s·ª≠ d·ª•ng markdown, kh√¥ng d√πng k√≠ hi·ªáu ƒë·∫∑c bi·ªát, kh√¥ng c·∫ßn l·ªùi d·∫´n, ch·ªâ c·∫ßn tr·ªçng t√¢m n·ªôi dung"
            "Tr√°nh l·∫∑p l·∫°i t·ª´ ng·ªØ kh√¥ng c·∫ßn thi·∫øt v√† ƒë·∫£m b·∫£o kh√¥ng b·ªè s√≥t th√¥ng tin c·ªët l√µi.\n\n"
            f"{content.strip()}"
        )

        body = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }

        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=" + self.GEMINI_API_KEY
        headers = {"Content-Type": "application/json"}

        try:
            response = requests.post(url, headers=headers, json=body)
            response.raise_for_status()
            result = response.json()
            return result["candidates"][0]["content"]["parts"][0]["text"]
        except Exception as e:
            print(f"‚ùå L·ªói khi t√≥m t·∫Øt v·ªõi Gemini: {e}")
            return None

    def classify_topics_with_gemini(self, title, content, summary, original_topics):
        """Ph√¢n lo·∫°i topics s·ª≠ d·ª•ng Gemini AI"""
        # T·∫°o danh s√°ch t·∫•t c·∫£ categories v√† subcategories
        categories_text = ""
        for main_category, subcategories in self.TOPICS_CATEGORIES["categories"].items():
            categories_text += f"\n{main_category}:\n"
            for sub in subcategories:
                categories_text += f"  - {sub}\n"

        prompt = (
            "B·∫°n l√† m·ªôt chuy√™n gia ph√¢n lo·∫°i tin t·ª©c c√¥ng ngh·ªá. "
            "Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc th√¥ng tin b√†i vi·∫øt v√† ch·ªçn 1-3 topics ph√π h·ª£p nh·∫•t t·ª´ danh s√°ch c√≥ s·∫µn.\n\n"
            
            "TH√îNG TIN B√ÄI VI·∫æT:\n"
            f"Ti√™u ƒë·ªÅ: {title}\n"
            f"T√≥m t·∫Øt: {summary}\n"
            f"Topics g·ªëc t·ª´ website: {', '.join(original_topics) if original_topics else 'Kh√¥ng c√≥'}\n\n"
            
            "DANH S√ÅCH TOPICS C√ì S·∫¥N:" + categories_text + "\n\n"
            
            "Y√äUC·∫¶U:\n"
            "1. Ch·ªçn 1-3 topics con (subcategories) ph√π h·ª£p nh·∫•t\n"
            "2. Tr·∫£ v·ªÅ CH√çNH X√ÅC theo format JSON:\n"
            '{"selected_topics": ["topic1", "topic2", "topic3"]}\n\n'
            
            "3. CH·ªà ch·ªçn t·ª´ danh s√°ch c√≥ s·∫µn, KH√îNG t·ª± t·∫°o topics m·ªõi\n"
            "4. ∆Øu ti√™n topics c·ª• th·ªÉ h∆°n topics chung\n"
            "5. KH√îNG th√™m gi·∫£i th√≠ch, ch·ªâ tr·∫£ v·ªÅ JSON"
        )

        body = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }

        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=" + self.GEMINI_API_KEY
        headers = {"Content-Type": "application/json"}

        try:
            response = requests.post(url, headers=headers, json=body)
            response.raise_for_status()
            result = response.json()
            response_text = result["candidates"][0]["content"]["parts"][0]["text"]
            
            # Parse JSON response
            try:
                # T√¨m v√† extract JSON t·ª´ response
                import re
                json_match = re.search(r'\{[^{}]*"selected_topics"[^{}]*\}', response_text)
                if json_match:
                    json_data = json.loads(json_match.group())
                    return json_data.get("selected_topics", [])
                else:
                    print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y JSON trong response: {response_text}")
                    return []
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è L·ªói parse JSON: {e}")
                print(f"Response: {response_text}")
                return []
                
        except Exception as e:
            print(f"‚ùå L·ªói khi ph√¢n lo·∫°i topics v·ªõi Gemini: {e}")
            return []

    def add_classified_topics_to_articles(self, articles_file="articles_summarize.json", output_file="articles_with_topics.json"):
        """Th√™m topics ƒë∆∞·ª£c ph√¢n lo·∫°i v√†o c√°c b√†i vi·∫øt"""
        try:
            # Load articles
            with open(articles_file, "r", encoding="utf-8") as f:
                articles = json.load(f)
            
            updated_articles = []
            
            for i, article in enumerate(articles):
                print(f"üîç ƒêang ph√¢n lo·∫°i b√†i {i+1}/{len(articles)}: {article.get('title', 'Unknown')[:50]}...")
                
                # Extract th√¥ng tin c·∫ßn thi·∫øt
                title = article.get("title", "")
                content = article.get("content", "")
                summary = article.get("summary", "")
                original_topics = article.get("topic", [])
                
                # Ph√¢n lo·∫°i topics
                classified_topics = self.classify_topics_with_gemini(title, content, summary, original_topics)
                
                # Th√™m v√†o article
                article_copy = article.copy()
                article_copy["classified_topics"] = classified_topics
                article_copy["classification_timestamp"] = datetime.now().isoformat()
                
                updated_articles.append(article_copy)
                
                print(f"‚úÖ Topics ƒë∆∞·ª£c ch·ªçn: {', '.join(classified_topics) if classified_topics else 'Kh√¥ng c√≥'}")
                
                # Delay ƒë·ªÉ tr√°nh rate limit
                time.sleep(2)
            
            # L∆∞u k·∫øt qu·∫£
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(updated_articles, f, ensure_ascii=False, indent=4)
            
            print(f"‚úÖ ƒê√£ ho√†n th√†nh ph√¢n lo·∫°i v√† l∆∞u v√†o {output_file}")
            return len(updated_articles)
            
        except Exception as e:
            print(f"‚ùå L·ªói khi ph√¢n lo·∫°i topics: {e}")
            return 0

    def classify_single_article_topics(self, article_file="test_summary.json"):
        """Ph√¢n lo·∫°i topics cho m·ªôt b√†i vi·∫øt ƒë∆°n l·∫ª"""
        try:
            # Load article
            with open(article_file, "r", encoding="utf-8") as f:
                articles = json.load(f)
            
            if not articles:
                print("‚ùå Kh√¥ng c√≥ b√†i vi·∫øt n√†o trong file!")
                return None
                
            article = articles[0] if isinstance(articles, list) else articles
            
            print(f"üîç ƒêang ph√¢n lo·∫°i b√†i vi·∫øt: {article.get('title', 'Unknown')[:50]}...")
            
            # Extract th√¥ng tin
            title = article.get("title", "")
            content = article.get("content", "")
            summary = article.get("summary", "")
            original_topics = article.get("topic", [])
            
            # Ph√¢n lo·∫°i
            classified_topics = self.classify_topics_with_gemini(title, content, summary, original_topics)
            
            # Th√™m topics v√†o article
            article["classified_topics"] = classified_topics
            article["classification_timestamp"] = datetime.now().isoformat()
            
            # L∆∞u l·∫°i
            with open("test_summary_with_topics.json", "w", encoding="utf-8") as f:
                json.dump([article], f, ensure_ascii=False, indent=4)
            
            print(f"‚úÖ Topics ƒë∆∞·ª£c ch·ªçn: {', '.join(classified_topics) if classified_topics else 'Kh√¥ng c√≥'}")
            print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o test_summary_with_topics.json")
            
            return classified_topics
            
        except Exception as e:
            print(f"‚ùå L·ªói khi ph√¢n lo·∫°i topics: {e}")
            return None

    def summarize(self, soup, config):
        """T√≥m t·∫Øt b√†i vi·∫øt v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin"""
        try:
            title_tag = soup.select_one(config["title"])
            content_tags = soup.select(config["content"])
            image_tags = soup.select(config["images"])
            author_tag = soup.select_one(config["author"])
            time_tag = soup.select_one(config["time"])
            topics_tag = soup.select(config["topic"])
            references_tags = soup.select(config["references"])
            
            title = title_tag.text.strip() if title_tag else None
            content = "\n".join(p.text.strip() for p in content_tags if p.text.strip())
            summ = self.summarize_with_gemini(content)
            images = [img["src"] for img in image_tags if img.get("src")]
            author = author_tag.text.strip() if author_tag else None
            time = time_tag.text.strip() if time_tag else None
            topics = [topic.get_text(strip=True) for topic in topics_tag]
            references = [link['href'] for link in references_tags if link.get('href')]
            
            return {
                "title": title,
                "content": content,
                "images": images,
                "author": author,
                "time": time,
                "topic": topics,
                "references": references,
                "summary": summ,
            }
        except Exception as e:
            print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t d·ªØ li·ªáu: {e}")
            return None

    def parser_seo(self, soup, config):
        """Ph√¢n t√≠ch d·ªØ li·ªáu SEO c·ªßa b√†i vi·∫øt"""
        try:
            seo_data = {
                "meta_title": "",
                "meta_description": "",
                "meta_keywords": [],
                "h1": "",
                "h2": [],
                "canonical_url": "",
                "word_count": 0,
                "internal_links": [],
                "external_links": []
            }
            
            meta_title = soup.find("meta", property="og:title") or soup.find("meta", attrs={"name": "title"})
            if meta_title and meta_title.get("content"):
                seo_data["meta_title"] = meta_title["content"]
                
            meta_desc = soup.find("meta", property="og:description") or soup.find("meta", attrs={"name": "description"})
            if meta_desc and meta_desc.get("content"):
                seo_data["meta_description"] = meta_desc["content"]
                
            meta_keywords = soup.find("meta", attrs={"name": "keywords"})
            if meta_keywords and meta_keywords.get("content"):
                seo_data["meta_keywords"] = [kw.strip() for kw in meta_keywords["content"].split(",")]
                
            h1_tag = soup.find("h1")
            if h1_tag:
                seo_data["h1"] = h1_tag.get_text(strip=True)
                
            h2_tags = soup.find_all("h2")
            if h2_tags:
                seo_data["h2"] = [h2.get_text(strip=True) for h2 in h2_tags]
                
            canonical = soup.find("link", rel="canonical")
            if canonical and canonical.get("href"):
                seo_data["canonical_url"] = canonical["href"]
                
            content_container = soup.select_one(config.get("content", ""))
            if content_container:
                text_content = " ".join(p.get_text(strip=True) for p in content_container.find_all("p"))
                seo_data["word_count"] = len(text_content.split())

            domain = config["domain"]
            links = soup.select(config.get("references", ""))
            for link in links:
                href = link.get("href")
                if href:
                    if domain in href or href.startswith("/"):
                        seo_data["internal_links"].append(href)
                    else:
                        seo_data["external_links"].append(href)

            return seo_data
        except Exception as e:
            print(f"‚ùå L·ªói khi ph√¢n t√≠ch SEO: {e}")
            return None

    def create_driver(self):
        """T·∫°o Chrome driver"""
        options = Options()
        options.headless = True
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        return driver

    def load_existing_data(self, filename):
        """Load d·ªØ li·ªáu hi·ªán c√≥ t·ª´ file JSON"""
        try:
            with open(filename, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc {filename}: {e}")
            return []

    def is_duplicate_article(self, new_article, existing_articles):
        """Ki·ªÉm tra b√†i vi·∫øt c√≥ tr√πng l·∫∑p kh√¥ng"""
        if not new_article or not new_article.get("title"):
            return True
            
        new_title = new_article["title"].strip().lower()
        
        for existing in existing_articles:
            if not existing or not existing.get("title"):
                continue
                
            existing_title = existing["title"].strip().lower()
            
            # Ki·ªÉm tra ti√™u ƒë·ªÅ gi·ªëng nhau
            if new_title == existing_title:
                return True
                
            # Ki·ªÉm tra ƒë·ªô t∆∞∆°ng ƒë·ªìng cao (85% tr·ªü l√™n)
            similarity = self.calculate_similarity(new_title, existing_title)
            if similarity >= 0.85:
                return True
                
        return False

    def calculate_similarity(self, text1, text2):
        """T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 chu·ªói"""
        if not text1 or not text2:
            return 0
            
        # S·ª≠ d·ª•ng Jaccard similarity v·ªõi t·ª´
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        if not union:
            return 0
            
        return len(intersection) / len(union)

    def save_data_incrementally(self, new_articles, new_summaries, new_seo_data):
        """L∆∞u d·ªØ li·ªáu m·ªõi v√†o file hi·ªán c√≥"""
        # Load d·ªØ li·ªáu hi·ªán c√≥
        existing_articles = self.load_existing_data("articles.json")
        existing_summaries = self.load_existing_data("articles_summarize.json")
        existing_seo = self.load_existing_data("seo_data.json")
        
        # Filter b√†i vi·∫øt m·ªõi kh√¥ng tr√πng l·∫∑p
        filtered_articles = []
        filtered_summaries = []
        filtered_seo = []
        
        for i, article in enumerate(new_articles):
            if not self.is_duplicate_article(article, existing_articles):
                filtered_articles.append(article)
                if i < len(new_summaries):
                    filtered_summaries.append(new_summaries[i])
                if i < len(new_seo_data):
                    filtered_seo.append(new_seo_data[i])
            else:
                print(f"‚ö†Ô∏è B·ªè qua b√†i tr√πng l·∫∑p: {article.get('title', 'Unknown')[:50]}...")
        
        # Th√™m v√†o d·ªØ li·ªáu hi·ªán c√≥
        existing_articles.extend(filtered_articles)
        existing_summaries.extend(filtered_summaries)
        existing_seo.extend(filtered_seo)
        
        # L∆∞u l·∫°i file
        with open("articles.json", "w", encoding="utf-8") as f:
            json.dump(existing_articles, f, ensure_ascii=False, indent=4)
        with open("articles_summarize.json", "w", encoding="utf-8") as f:
            json.dump(existing_summaries, f, ensure_ascii=False, indent=4)
        with open("seo_data.json", "w", encoding="utf-8") as f:
            json.dump(existing_seo, f, ensure_ascii=False, indent=4)
            
        return len(filtered_articles)

    def get_seo_inf(self, url):
        """L·∫•y th√¥ng tin SEO t·ª´ URL"""
        try:
            driver = self.create_driver()
            driver.get(url)
            time.sleep(3)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            driver.quit()
            
            domain = urlparse(url).netloc
            config = self.get_parser_by_domain(domain)

            if not config:
                print(f"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}")
                return None

            seo_data = self.parser_seo(soup, config)
            if not seo_data:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ thu th·∫≠p d·ªØ li·ªáu SEO cho: {url}")
                return None
                
            output_file = "seo_data.json"
            try:
                with open(output_file, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
            except FileNotFoundError:
                existing_data = []

            existing_data.append({
                "url": url,
                "domain": domain,
                "seo_data": seo_data
            })

            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=4)

            return seo_data
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω URL {url}: {e}")
            return None

    def process_article(self, url):
        """X·ª≠ l√Ω m·ªôt b√†i vi·∫øt t·ª´ URL"""
        try:
            driver = self.create_driver()
            driver.get(url)
            time.sleep(3)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            driver.quit()
            
            domain = urlparse(url).netloc 
            config = self.get_parser_by_domain(domain)
            config_links = self.get_parser_link(domain)
            
            if not config:
                print(f"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}")
                return None

            if not config_links:
                print(f"‚ùå Kh√¥ng c√≥ parser_links cho domain: {domain}")
                return None

            links = self.get_latest_news(soup, config_links)
            article = self.extract_by_config(soup, config)
            summarize_article = self.summarize(soup, config)
            seo_data = self.parser_seo(soup, config)
            
            # Th√™m ph√¢n lo·∫°i topics t·ª± ƒë·ªông
            if summarize_article and summarize_article.get("title") and summarize_article.get("summary"):
                print("üéØ ƒêang ph√¢n lo·∫°i topics...")
                classified_topics = self.classify_topics_with_gemini(
                    summarize_article.get("title", ""),
                    summarize_article.get("content", ""),
                    summarize_article.get("summary", ""),
                    summarize_article.get("topic", [])
                )
                
                # Th√™m topics v√†o c·∫£ hai article objects
                if article:
                    article["rcm_topics"] = classified_topics
                if summarize_article:
                    summarize_article["rcm_topics"] = classified_topics
                
                print(f"‚úÖ Topics g·ª£i √Ω: {', '.join(classified_topics) if classified_topics else 'Kh√¥ng c√≥'}")
            
            return article, summarize_article, seo_data
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω URL {url}: {e}")   
            return None

    def crawl_multiple_urls(self):
        """Crawl nhi·ªÅu URL t·ª´ c√°c domain ƒë√£ c·∫•u h√¨nh"""
        try:
            articles = []
            summaries = []
            seo_data_list = []
            domains = [config["domain"] for config in self.PARSERS.values()]
            
            for domain in domains:
                config_links = self.get_parser_link(domain)
                if not config_links:
                    print(f"‚ùå Kh√¥ng c√≥ parser_links cho domain: {domain}")
                    continue
                    
                driver = self.create_driver()
                driver.get(f"https://{domain}")
                time.sleep(3)
                soup = BeautifulSoup(driver.page_source, "html.parser")
                driver.quit()
                
                links = self.get_latest_news(soup, config_links)
                for url in links[:2]:  # L·∫•y 2 b√†i m·ªõi nh·∫•t
                    if url.startswith("/"):
                        url = f"https://{domain}{url}"

                    result = self.process_article(url)
                    if result:
                        article, summarize_article, seo_data = result
                        articles.append(article)
                        summaries.append(summarize_article)
                        seo_data_list.append({
                            "url": url,
                            "domain": domain,
                            "seo_data": seo_data
                        })
                        
            # L∆∞u k·∫øt qu·∫£ incremental (ghi ti·∫øp v√†o file hi·ªán c√≥)
            new_articles_count = self.save_data_incrementally(articles, summaries, seo_data_list)
                
            print(f"‚úÖ Ho√†n th√†nh crawl l√∫c {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"üìä S·ªë b√†i vi·∫øt m·ªõi thu th·∫≠p: {new_articles_count}/{len(articles)}")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi crawl multiple URLs: {e}")

    def start_crawling(self, interval_minutes=1):
        """B·∫Øt ƒë·∫ßu crawl t·ª± ƒë·ªông theo l·ªãch"""
        schedule.every(interval_minutes).minutes.do(self.crawl_multiple_urls)
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu l·ªãch crawl t·ª± ƒë·ªông m·ªói {interval_minutes} ph√∫t...")
        print("üìã Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng...")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(1)
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è ƒê√£ d·ª´ng crawling!")

    def crawl_single_url(self, url):
        """Crawl m·ªôt URL ƒë∆°n l·∫ª (ƒë·ªÉ test)"""
        print(f"üîç ƒêang crawl: {url}")
        result = self.process_article(url)
        
        if result:
            article, summarize_article, seo_data = result
            
            # L∆∞u k·∫øt qu·∫£ test v·ªõi topics
            with open("test_article.json", "w", encoding="utf-8") as f:
                json.dump([article], f, ensure_ascii=False, indent=4)
            with open("test_summary.json", "w", encoding="utf-8") as f:
                json.dump([summarize_article], f, ensure_ascii=False, indent=4)
            with open("test_seo.json", "w", encoding="utf-8") as f:
                json.dump([seo_data], f, ensure_ascii=False, indent=4)
                
            print("‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ test (bao g·ªìm topics g·ª£i √Ω)!")
            return result
        else:
            print("‚ùå Kh√¥ng th·ªÉ crawl URL n√†y!")
            return None


def main():
    """H√†m ch√≠nh"""
    crawler = NewsCrawler()
    
    print("ü§ñ News Crawler - H·ªá th·ªëng thu th·∫≠p tin t·ª©c t·ª± ƒë·ªông")
    print("=" * 60)
    print("1. üîÑ Crawl t·ª± ƒë·ªông theo l·ªãch + AI Topics")
    print("2. üöÄ Crawl t·∫•t c·∫£ domain + AI Topics") 
    print("3. üîç Crawl m·ªôt URL + AI Topics")

    
    choice = input("\nCh·ªçn ch·ª©c nƒÉng (1-3): ").strip()
    
    if choice == "1":
        interval = input("Nh·∫≠p kho·∫£ng th·ªùi gian (ph√∫t, m·∫∑c ƒë·ªãnh 1): ").strip()
        interval = int(interval) if interval.isdigit() else 1
        print("üîÑ B·∫Øt ƒë·∫ßu crawl t·ª± ƒë·ªông v·ªõi AI topics classification...")
        crawler.start_crawling(interval)
        
    elif choice == "2":
        print("üöÄ B·∫Øt ƒë·∫ßu crawl t·∫•t c·∫£ domain v·ªõi AI topics classification...")
        crawler.crawl_multiple_urls()
        
    elif choice == "3":
        url = input("Nh·∫≠p URL: ").strip()
        if url:
            print("üîç Crawl URL v·ªõi AI topics classification...")
            crawler.crawl_single_url(url)
        else:
            print("‚ùå URL kh√¥ng h·ª£p l·ªá!")
            
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")


if __name__ == "__main__":
    main()
