{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c01536f-aaf7-4e30-9dbe-1df6cd062b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parser_config = {\n",
    "    \"techcrunch\": {\n",
    "        \"domain\": \"techcrunch.com\",\n",
    "        \"title\": \"div.article-hero__middle\",\n",
    "        \"content\": \"div.entry-content\",\n",
    "        \"images\": \"div.entry-content img\",\n",
    "        \"author\": \"a.wp-block-tc23-author-card-name__link\",\n",
    "        \"time\": \"time\",\n",
    "        \"highlight\": \"a nofollow\",\n",
    "        \"topic\": \"div.tc23-post-relevant-terms__terms a\",\n",
    "        \"references\": \"div.entry-content a\"\n",
    "    },\n",
    "    \"vnexpress\": {\n",
    "        \"domain\": \"vnexpress.net\",\n",
    "        \"title\": \"h1.title-detail\",\n",
    "        \"content\": \"div.sidebar-1\",\n",
    "        \"images\": \"article.fck_detail img\",\n",
    "        \"author\": \"div.sidebar-1 p.Normal\",\n",
    "        \"time\": \"div.sidebar-1 span.date\",\n",
    "        \"highlight\": \"\",\n",
    "        \"topic\": \"ul.breadcrumb\",\n",
    "        \"references\": \"div.width_common box-tinlienquanv2\",\n",
    "    },\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\",\n",
    "        \"author\": \"\",\n",
    "        \"time\": \"\",\n",
    "        \"highlight\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"references\": \"\",\n",
    "    },\n",
    "    \"vietnamnet\": {\n",
    "        \"domain\": \"https://vietnamnet.vn\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\",\n",
    "        \"author\": \"\",\n",
    "        \"time\": \"\",\n",
    "        \"highlight\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"references\": \"\",\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"parsers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parser_config, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73bb3b9b-f252-4f80-8d82-070a1ea5fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_links = {\n",
    "    \"techcrunch\":{\n",
    "        \"domain\": \"techcrunch.com\",\n",
    "        \"links\": \"a.loop-card__title-link\",\n",
    "    },\n",
    "    \"vnexpress\":{\n",
    "        \"domain\": \"vnexpress.net\",\n",
    "        \"links\": \"wrapper-topstory-folder flexbox width_common wrapper-topstory-folder-v2 no-border a\"\n",
    "    }\n",
    "}\n",
    "with open(\"parsers_links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parser_links, f, ensure_ascii=False, indent = 4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88ae3562-aa25-42ea-a087-b22ece93de63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import schedule\n",
    "\n",
    "with open(\"parsers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS = json.load(f)\n",
    "\n",
    "with open(\"parsers_links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS_LINKS = json.load(f)\n",
    "\n",
    "def get_parser_by_domain(domain):\n",
    "    for config in PARSERS.values():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def get_parser_link(domain):\n",
    "    for config in PARSERS_LINKS.values(): \n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def get_latest_news(soup, config):\n",
    "    links_tags = soup.select(config[\"links\"])\n",
    "    print(links_tags)\n",
    "    links = [a['href'] for a in links_tags]\n",
    "    return links\n",
    "\n",
    "def extract_by_config(soup, config):\n",
    "    result = {\n",
    "        \"title\": \"\",\n",
    "        \"author\": \"\",\n",
    "        \"time\": \"\",\n",
    "        \"topics\": [],\n",
    "        \"content\": []\n",
    "    }\n",
    "    \n",
    "    title_tag = soup.select_one(config.get(\"title\", \"\"))\n",
    "    if title_tag:\n",
    "        result[\"title\"] = title_tag.get_text(strip=True)\n",
    "        \n",
    "    author_tag = soup.select_one(config.get(\"author\", \"\"))\n",
    "    if author_tag:\n",
    "        result[\"author\"] = author_tag.get_text(strip=True)\n",
    "        \n",
    "    time_tag = soup.select_one(config.get(\"time\", \"\"))\n",
    "    if time_tag:\n",
    "        result[\"time\"] = time_tag.get_text(strip=True)\n",
    "        \n",
    "    topic_tags = soup.select(config.get(\"topic\", \"\"))\n",
    "    if topic_tags:\n",
    "        result[\"topics\"] = [tag.get_text(strip=True) for tag in topic_tags]\n",
    "        \n",
    "    content_container = soup.select_one(config.get(\"content\", \"\"))\n",
    "    if content_container:\n",
    "        all_elements = content_container.find_all(recursive=True)\n",
    "        for element in all_elements:\n",
    "            if element.name == \"p\" and element.get_text(strip=True):\n",
    "                result[\"content\"].append({\"type\": \"text\", \"value\": element.get_text(strip=True)})\n",
    "            elif element.name == \"img\" and element.get(\"src\", \"\"):\n",
    "                result[\"content\"].append({\"type\": \"image\", \"value\": element.get(\"src\")})\n",
    "            elif element.name == \"a\" and element.get(\"href\", \"\"):\n",
    "                result[\"content\"].append({\"type\": \"link\", \"value\": element.get(\"href\")})\n",
    "                \n",
    "    return result\n",
    "\n",
    "def summarize(soup, config):\n",
    "    try:\n",
    "        title_tag = soup.select_one(config[\"title\"])\n",
    "        content_tags = soup.select(config[\"content\"])\n",
    "        image_tags = soup.select(config[\"images\"])\n",
    "        author_tag = soup.select_one(config[\"author\"])\n",
    "        time_tag = soup.select_one(config[\"time\"])\n",
    "        topics_tag = soup.select(config[\"topic\"])\n",
    "        references_tags = soup.select(config[\"references\"])\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "        content = \"\\n\".join(p.text.strip() for p in content_tags if p.text.strip())\n",
    "        images = [img[\"src\"] for img in image_tags if img.get(\"src\")]\n",
    "        author = author_tag.text.strip() if author_tag else None\n",
    "        time = time_tag.text.strip() if time_tag else None\n",
    "        topics = [topic.get_text(strip=True) for topic in topics_tag]\n",
    "        references = href_list = [link['href'] for link in references_tags if link.get('href')]\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"images\": images,\n",
    "            \"author\": author,\n",
    "            \"time\": time,\n",
    "            \"topic\": topics,\n",
    "            \"references\": references,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi tr√≠ch xu·∫•t d·ªØ li·ªáu: {e}\")\n",
    "        return None\n",
    "\n",
    "def parser_seo(soup, config):\n",
    "    try:\n",
    "        seo_data = {\n",
    "            \"meta_title\": \"\",\n",
    "            \"meta_description\": \"\",\n",
    "            \"meta_keywords\": [],\n",
    "            \"h1\": \"\",\n",
    "            \"h2\": [],\n",
    "            \"canonical_url\": \"\",\n",
    "            \"word_count\": 0,\n",
    "            \"internal_links\": [],\n",
    "            \"external_links\": []\n",
    "        }\n",
    "        meta_title = soup.find(\"meta\", property=\"og:title\") or soup.find(\"meta\", attrs={\"name\": \"title\"})\n",
    "        if meta_title and meta_title.get(\"content\"):\n",
    "            seo_data[\"meta_title\"] = meta_title[\"content\"]\n",
    "        meta_desc = soup.find(\"meta\", property=\"og:description\") or soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        if meta_desc and meta_desc.get(\"content\"):\n",
    "            seo_data[\"meta_description\"] = meta_desc[\"content\"]\n",
    "        meta_keywords = soup.find(\"meta\", attrs={\"name\": \"keywords\"})\n",
    "        if meta_keywords and meta_keywords.get(\"content\"):\n",
    "            seo_data[\"meta_keywords\"] = [kw.strip() for kw in meta_keywords[\"content\"].split(\",\")]\n",
    "        h1_tag = soup.find(\"h1\")\n",
    "        if h1_tag:\n",
    "            seo_data[\"h1\"] = h1_tag.get_text(strip=True)\n",
    "        h2_tags = soup.find_all(\"h2\")\n",
    "        if h2_tags:\n",
    "            seo_data[\"h2\"] = [h2.get_text(strip=True) for h2 in h2_tags]\n",
    "        canonical = soup.find(\"link\", rel=\"canonical\")\n",
    "        if canonical and canonical.get(\"href\"):\n",
    "            seo_data[\"canonical_url\"] = canonical[\"href\"]\n",
    "        content_container = soup.select_one(config.get(\"content\", \"\"))\n",
    "        if content_container:\n",
    "            text_content = \" \".join(p.get_text(strip=True) for p in content_container.find_all(\"p\"))\n",
    "            seo_data[\"word_count\"] = len(text_content.split())\n",
    "\n",
    "        domain = config[\"domain\"]\n",
    "        links = soup.select(config.get(\"references\", \"\"))\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if href:\n",
    "                if domain in href or href.startswith(\"/\"):\n",
    "                    seo_data[\"internal_links\"].append(href)\n",
    "                else:\n",
    "                    seo_data[\"external_links\"].append(href)\n",
    "\n",
    "        return seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi ph√¢n t√≠ch SEO: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_seo_inf(url):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        driver.quit()\n",
    "        domain = urlparse(url).netloc\n",
    "        config = get_parser_by_domain(domain)\n",
    "\n",
    "        if not config:\n",
    "            print(f\"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        seo_data = parser_seo(soup, config)\n",
    "        if not seo_data:\n",
    "            print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ thu th·∫≠p d·ªØ li·ªáu SEO cho: {url}\")\n",
    "            return None\n",
    "        output_file = \"seo_data.json\"\n",
    "        try:\n",
    "            with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            existing_data = []\n",
    "\n",
    "        existing_data.append({\n",
    "            \"url\": url,\n",
    "            \"domain\": domain,\n",
    "            \"seo_data\": seo_data\n",
    "        })\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        return seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi x·ª≠ l√Ω URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_article(url):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        driver.quit()\n",
    "        domain = urlparse(url).netloc \n",
    "        config = get_parser_by_domain(domain)\n",
    "        config_links = get_parser_link(domain)\n",
    "        \n",
    "        if not config:\n",
    "            print(f\"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        if not config_links:\n",
    "            print(f\"‚ùå Kh√¥ng c√≥ parser_links cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        links = get_latest_news(soup, config_links)\n",
    "        print(links)\n",
    "        article = extract_by_config(soup, config)\n",
    "        summarize_article = summarize(soup, config)\n",
    "        seo_data = parser_seo(soup, config)\n",
    "        \n",
    "        if not article or not article[\"title\"] or not article[\"content\"]:\n",
    "            print(f\"‚ö†Ô∏è B·ªè qua v√¨ thi·∫øu ti√™u ƒë·ªÅ ho·∫∑c n·ªôi dung: {url}\")\n",
    "            return None\n",
    "            \n",
    "        return article, summarize_article, seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi x·ª≠ l√Ω URL {url}: {e}\")   \n",
    "        return None\n",
    "\n",
    "def crawl_multiple_urls():\n",
    "    try:\n",
    "        articles = []\n",
    "        summaries = []\n",
    "        seo_data_list = []\n",
    "        domains = [config[\"domain\"] for config in PARSERS.values()]\n",
    "        for domain in domains:\n",
    "            config_links = get_parser_link(domain)\n",
    "            if not config_links:\n",
    "                print(f\"‚ùå Kh√¥ng c√≥ parser_links cho domain: {domain}\")\n",
    "                continue\n",
    "            options = Options()\n",
    "            options.headless = True\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "            driver.get(f\"https://{domain}\")\n",
    "            time.sleep(3)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            driver.quit()\n",
    "            links = get_latest_news(soup, config_links)\n",
    "            for url in links[:2]:\n",
    "                if url.startswith(\"/\"):\n",
    "                    url = f\"https://{domain}{url}\"\n",
    "\n",
    "                result = process_article(url)\n",
    "                if result:\n",
    "                    article, summarize_article, seo_data = result\n",
    "                    articles.append(article)\n",
    "                    summaries.append(summarize_article)\n",
    "                    seo_data_list.append({\n",
    "                        \"url\": url,\n",
    "                        \"domain\": domain,\n",
    "                        \"seo_data\": seo_data\n",
    "                    })\n",
    "        with open(\"articles_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "        with open(\"articles_summarize.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(summaries, f, ensure_ascii=False, indent=4)\n",
    "        with open(\"seo_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(seo_data_list, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"‚úÖ Ho√†n th√†nh crawl l√∫c {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi crawl multiple URLs: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://vnexpress.net/tau-nang-luong-mat-troi-co-the-cho-7-000-chiec-xe-4910902.html'\n",
    "    # url = url = \"https://techcrunch.com/2025/07/07/threads-is-nearing-xs-daily-app-users-new-data-shows/\"\n",
    "    article, summarize_article, seo_data = process_article(url)\n",
    "    article = [article] if article else []\n",
    "    summarize_article = [summarize_article] if summarize_article else []\n",
    "    seo_data = [seo_data] if seo_data else []\n",
    "\n",
    "    # print(article)\n",
    "    # print(summarize_article)\n",
    "    # print(seo_data)\n",
    "    \n",
    "    # with open(\"articles_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(article, f, ensure_ascii=False, indent=4)\n",
    "    # with open(\"articles_summarize.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(summarize_article, f, ensure_ascii=False, indent=4)\n",
    "    # with open(\"seo_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(seo_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43529242-1e04-49a0-9094-0a18f2123439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu l·ªãch crawl t·ª± ƒë·ªông m·ªói 1 ph√∫t...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import schedule\n",
    "\n",
    "with open(\"parsers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS = json.load(f)\n",
    "\n",
    "with open(\"parsers_links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS_LINKS = json.load(f)\n",
    "\n",
    "def get_parser_by_domain(domain):\n",
    "    for config in PARSERS.values():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def get_parser_link(domain):\n",
    "    for config in PARSERS_LINKS.values():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def get_latest_news(soup, config):\n",
    "    links_tags = soup.select(config[\"links\"])\n",
    "    links = [a['href'] for a in links_tags]\n",
    "    return links\n",
    "\n",
    "def extract_by_config(soup, config):\n",
    "    result = {\n",
    "        \"title\": \"\",\n",
    "        \"author\": \"\",\n",
    "        \"time\": \"\",\n",
    "        \"topics\": [],\n",
    "        \"content\": []\n",
    "    }\n",
    "    \n",
    "    title_tag = soup.select_one(config.get(\"title\", \"\"))\n",
    "    if title_tag:\n",
    "        result[\"title\"] = title_tag.get_text(strip=True)\n",
    "        \n",
    "    author_tag = soup.select_one(config.get(\"author\", \"\"))\n",
    "    if author_tag:\n",
    "        result[\"author\"] = author_tag.get_text(strip=True)\n",
    "        \n",
    "    time_tag = soup.select_one(config.get(\"time\", \"\"))\n",
    "    if time_tag:\n",
    "        result[\"time\"] = time_tag.get_text(strip=True)\n",
    "        \n",
    "    topic_tags = soup.select(config.get(\"topic\", \"\"))\n",
    "    if topic_tags:\n",
    "        result[\"topics\"] = [tag.get_text(strip=True) for tag in topic_tags]\n",
    "        \n",
    "    content_container = soup.select_one(config.get(\"content\", \"\"))\n",
    "    if content_container:\n",
    "        all_elements = content_container.find_all(recursive=True)\n",
    "        for element in all_elements:\n",
    "            if element.name == \"p\" and element.get_text(strip=True):\n",
    "                result[\"content\"].append({\"type\": \"text\", \"value\": element.get_text(strip=True)})\n",
    "            elif element.name == \"img\" and element.get(\"src\", \"\"):\n",
    "                result[\"content\"].append({\"type\": \"image\", \"value\": element.get(\"src\")})\n",
    "            elif element.name == \"a\" and element.get(\"href\", \"\"):\n",
    "                result[\"content\"].append({\"type\": \"link\", \"value\": element.get(\"href\")})\n",
    "                \n",
    "    return result\n",
    "\n",
    "def summarize(soup, config):\n",
    "    try:\n",
    "        title_tag = soup.select_one(config[\"title\"])\n",
    "        content_tags = soup.select(config[\"content\"])\n",
    "        image_tags = soup.select(config[\"images\"])\n",
    "        author_tag = soup.select_one(config[\"author\"])\n",
    "        time_tag = soup.select_one(config[\"time\"])\n",
    "        topics_tag = soup.select(config[\"topic\"])\n",
    "        references_tags = soup.select(config[\"references\"])\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "        content = \"\\n\".join(p.text.strip() for p in content_tags if p.text.strip())\n",
    "        images = [img[\"src\"] for img in image_tags if img.get(\"src\")]\n",
    "        author = author_tag.text.strip() if author_tag else None\n",
    "        time = time_tag.text.strip() if time_tag else None\n",
    "        topics = [topic.get_text(strip=True) for topic in topics_tag]\n",
    "        references = href_list = [link['href'] for link in references_tags if link.get('href')]\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"images\": images,\n",
    "            \"author\": author,\n",
    "            \"time\": time,\n",
    "            \"topic\": topics,\n",
    "            \"references\": references,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi tr√≠ch xu·∫•t d·ªØ li·ªáu: {e}\")\n",
    "        return None\n",
    "\n",
    "def parser_seo(soup, config):\n",
    "    try:\n",
    "        seo_data = {\n",
    "            \"meta_title\": \"\",\n",
    "            \"meta_description\": \"\",\n",
    "            \"meta_keywords\": [],\n",
    "            \"h1\": \"\",\n",
    "            \"h2\": [],\n",
    "            \"canonical_url\": \"\",\n",
    "            \"word_count\": 0,\n",
    "            \"internal_links\": [],\n",
    "            \"external_links\": []\n",
    "        }\n",
    "        meta_title = soup.find(\"meta\", property=\"og:title\") or soup.find(\"meta\", attrs={\"name\": \"title\"})\n",
    "        if meta_title and meta_title.get(\"content\"):\n",
    "            seo_data[\"meta_title\"] = meta_title[\"content\"]\n",
    "        meta_desc = soup.find(\"meta\", property=\"og:description\") or soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        if meta_desc and meta_desc.get(\"content\"):\n",
    "            seo_data[\"meta_description\"] = meta_desc[\"content\"]\n",
    "        meta_keywords = soup.find(\"meta\", attrs={\"name\": \"keywords\"})\n",
    "        if meta_keywords and meta_keywords.get(\"content\"):\n",
    "            seo_data[\"meta_keywords\"] = [kw.strip() for kw in meta_keywords[\"content\"].split(\",\")]\n",
    "        h1_tag = soup.find(\"h1\")\n",
    "        if h1_tag:\n",
    "            seo_data[\"h1\"] = h1_tag.get_text(strip=True)\n",
    "        h2_tags = soup.find_all(\"h2\")\n",
    "        if h2_tags:\n",
    "            seo_data[\"h2\"] = [h2.get_text(strip=True) for h2 in h2_tags]\n",
    "        canonical = soup.find(\"link\", rel=\"canonical\")\n",
    "        if canonical and canonical.get(\"href\"):\n",
    "            seo_data[\"canonical_url\"] = canonical[\"href\"]\n",
    "        content_container = soup.select_one(config.get(\"content\", \"\"))\n",
    "        if content_container:\n",
    "            text_content = \" \".join(p.get_text(strip=True) for p in content_container.find_all(\"p\"))\n",
    "            seo_data[\"word_count\"] = len(text_content.split())\n",
    "\n",
    "        domain = config[\"domain\"]\n",
    "        links = soup.select(config.get(\"references\", \"\"))\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if href:\n",
    "                if domain in href or href.startswith(\"/\"):\n",
    "                    seo_data[\"internal_links\"].append(href)\n",
    "                else:\n",
    "                    seo_data[\"external_links\"].append(href)\n",
    "\n",
    "        return seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi ph√¢n t√≠ch SEO: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_seo_inf(url):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        driver.quit()\n",
    "        domain = urlparse(url).netloc\n",
    "        config = get_parser_by_domain(domain)\n",
    "\n",
    "        if not config:\n",
    "            print(f\"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        seo_data = parser_seo(soup, config)\n",
    "        if not seo_data:\n",
    "            print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ thu th·∫≠p d·ªØ li·ªáu SEO cho: {url}\")\n",
    "            return None\n",
    "        output_file = \"seo_data.json\"\n",
    "        try:\n",
    "            with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            existing_data = []\n",
    "\n",
    "        existing_data.append({\n",
    "            \"url\": url,\n",
    "            \"domain\": domain,\n",
    "            \"seo_data\": seo_data\n",
    "        })\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        return seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi x·ª≠ l√Ω URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_article(url):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        driver.quit()\n",
    "        domain = urlparse(url).netloc \n",
    "        config = get_parser_by_domain(domain)\n",
    "        config_links = get_parser_link(domain)\n",
    "        \n",
    "        if not config:\n",
    "            print(f\"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        if not config_links:\n",
    "            print(f\"‚ùå Kh√¥ng c√≥ parser_links cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        links = get_latest_news(soup, config_links)\n",
    "        article = extract_by_config(soup, config)\n",
    "        summarize_article = summarize(soup, config)\n",
    "        seo_data = parser_seo(soup, config)\n",
    "        \n",
    "        if not article or not article[\"title\"] or not article[\"content\"]:\n",
    "            print(f\"‚ö†Ô∏è B·ªè qua v√¨ thi·∫øu ti√™u ƒë·ªÅ ho·∫∑c n·ªôi dung: {url}\")\n",
    "            return None\n",
    "            \n",
    "        return article, summarize_article, seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi x·ª≠ l√Ω URL {url}: {e}\")   \n",
    "        return None\n",
    "\n",
    "def crawl_multiple_urls():\n",
    "    try:\n",
    "        articles = []\n",
    "        summaries = []\n",
    "        seo_data_list = []\n",
    "        domains = [config[\"domain\"] for config in PARSERS.values()]\n",
    "        for domain in domains:\n",
    "            config_links = get_parser_link(domain)\n",
    "            if not config_links:\n",
    "                print(f\"‚ùå Kh√¥ng c√≥ parser_links cho domain: {domain}\")\n",
    "                continue\n",
    "            options = Options()\n",
    "            options.headless = True\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "            driver.get(f\"https://{domain}\")\n",
    "            time.sleep(3)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            driver.quit()\n",
    "            links = get_latest_news(soup, config_links)\n",
    "            for url in links[:2]:\n",
    "                if url.startswith(\"/\"):\n",
    "                    url = f\"https://{domain}{url}\"\n",
    "\n",
    "                result = process_article(url)\n",
    "                if result:\n",
    "                    article, summarize_article, seo_data = result\n",
    "                    articles.append(article)\n",
    "                    summaries.append(summarize_article)\n",
    "                    seo_data_list.append({\n",
    "                        \"url\": url,\n",
    "                        \"domain\": domain,\n",
    "                        \"seo_data\": seo_data\n",
    "                    })\n",
    "        with open(\"articles_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "        with open(\"articles_summarize.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(summaries, f, ensure_ascii=False, indent=4)\n",
    "        with open(\"seo_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(seo_data_list, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"‚úÖ Ho√†n th√†nh crawl l√∫c {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi crawl multiple URLs: {e}\")\n",
    "\n",
    "def start_crawling():\n",
    "    schedule.every(1).minutes.do(crawl_multiple_urls)\n",
    "    \n",
    "    print(\"üöÄ B·∫Øt ƒë·∫ßu l·ªãch crawl t·ª± ƒë·ªông m·ªói 1 ph√∫t...\")\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_crawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe700e8-471e-4796-aee2-402ef8f039a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "with open(\"parsers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS = json.load(f)\n",
    "\n",
    "def get_parser_by_domain(domain):\n",
    "    for name, config in PARSERS.items():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def extract_by_config(soup, config):\n",
    "    title_tag = soup.select_one(config[\"title\"])\n",
    "    paragraphs = soup.select(config[\"content\"])\n",
    "    images = [img[\"src\"] for img in soup.select(config[\"images\"]) if img.get(\"src\")]\n",
    "\n",
    "    title = title_tag.text.strip() if title_tag else None\n",
    "    content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"images\": images\n",
    "    }\n",
    "\n",
    "def process_article(url, summarizer):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    domain = urlparse(url).netloc\n",
    "    config = get_parser_by_domain(domain)\n",
    "\n",
    "    if not config:\n",
    "        print(f\"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}\")\n",
    "        return None\n",
    "\n",
    "    article = extract_by_config(soup, config)\n",
    "\n",
    "    if not article[\"title\"] or not article[\"content\"].strip():\n",
    "        print(f\"‚ö†Ô∏è B·ªè qua v√¨ thi·∫øu ti√™u ƒë·ªÅ ho·∫∑c n·ªôi dung: {url}\")\n",
    "        return None\n",
    "\n",
    "    content_trimmed = article[\"content\"][:3000].strip()\n",
    "\n",
    "    try:\n",
    "        summary = summarizer(content_trimmed, max_length=200, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi t√≥m t·∫Øt b√†i vi·∫øt: {url}\\n{e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüì∞ URL:\", url)\n",
    "    print(\"üìå Ti√™u ƒë·ªÅ:\", article[\"title\"])\n",
    "    print(\"üñºÔ∏è ·∫¢nh:\", article[\"images\"] if article[\"images\"] else \"Kh√¥ng c√≥ ·∫£nh\")\n",
    "    print(\"\\nüìÑ N·ªôi dung:\\n\", article[\"content\"])\n",
    "    print(\"\\nüß† T√≥m t·∫Øt:\\n\", summary)\n",
    "\n",
    "    return {\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": url,\n",
    "        \"summary\": summary,\n",
    "        \"content\": article[\"content\"],\n",
    "        \"images\": \", \".join(article[\"images\"])\n",
    "    }\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "urls = [\n",
    "    \"https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\",\n",
    "    \"https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\",\n",
    "    \"https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for url in urls:\n",
    "    result = process_article(url, summarizer)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√£ l∆∞u v√†o file articles.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_kernel",
   "language": "python",
   "name": "my_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
