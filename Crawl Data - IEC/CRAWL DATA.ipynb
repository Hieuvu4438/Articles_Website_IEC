{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8456da-cbba-4200-8ee3-4a810ce6727d",
   "metadata": {},
   "source": [
    "**DEMO: Crawl Data t·ª´ Tech Crunch: L·∫•y ra c√°c th·∫ª href l√† c√°c ƒë∆∞·ªùng link d·∫´n t·ªõi trang tin t·ª©c t·ª´ Homepage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b8322e-131b-4c23-8ba1-c5f5f5ae7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "url = \"https://techcrunch.com/latest/\"\n",
    "\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "articles = soup.find_all(\"a\", class_=\"loop-card__title-link\")\n",
    "links = [a['href'] for a in articles]\n",
    "\n",
    "df = pd.DataFrame(links, columns=[\"Article URL\"])\n",
    "df.to_csv(\"techcrunch_latest_links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9e825-d014-46ed-b07e-dae4d482e2a2",
   "metadata": {},
   "source": [
    "**Crawl Data t·ª´ 1 trang web n∆∞·ªõc ngo√†i + sumamry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f3f1ff-403c-42de-8f7b-a673cb65e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 1000, but your input_length is only 417. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ti√™u ƒë·ªÅ: AWS enters into ‚Äòstrategic partnership‚Äô with Saudi Arabia-backed Humain\n",
      "URL: https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\n",
      "·∫¢nh: Kh√¥ng c√≥ ·∫£nh\n",
      "\n",
      "üì∞ N·ªôi dung b√†i vi·∫øt ƒë·∫ßy ƒë·ªß:\n",
      "\n",
      "Amazon says it will work with Humain, the AI company recently launched by Saudi Arabia‚Äôs ruler, Mohammed bin Salman, to invest ‚Äú$5 billion-plus‚Äù in a strategic partnership to build an ‚ÄúAI Zone‚Äù in Saudi Arabia.\n",
      "The AI Zone will include dedicated Amazon Web Services (AWS) AI infrastructure, servers, networks, and training and certification programs, according to a press release. Humain is pledging to develop AI solutions using AWS technologies and to work with AWS on providing access to tools and programs for Saudi Arabia-based AI startups.\n",
      "AWS joins tech giants Nvidia, AMD, and others in partnering with Humain, which is funded by Saudi Arabia‚Äôs Public Investment Fund (PIF). American tech firms have looked to the PIF as a source of capital. Companies like¬†Google¬†and¬†Salesforce¬†have also recently worked with the PIF on AI-related projects and investments.\n",
      "President Donald Trump¬†and several tech industry allies attended a U.S.-Saudi investment forum on Tuesday. Under a new Trump administration initiative, U.S.-based tech suppliers, including Nvidia and AMD, have been permitted to arrange deals with Saudi Arabian firms.\n",
      "Saudi Arabia has mandated AI companies and services in the kingdom store data locally, pushing vendors to put facilities there to avoid losing contracts. Both Google and Oracle have announced expansion plans in the region over the past year.\n",
      "Amazon early last March pledged to spend billions of dollars on data centers in Saudi Arabia. On Tuesday, the company said it is devoting around $5.3 billion to develop an AWS region in the kingdom scheduled to come online in 2026. The AI Zone commitments are an ‚Äúadditional investment‚Äù separate from the roughly $5.3 billion, Amazon said. It isn‚Äôt clear, however, if Amazon‚Äôs contribution to the AI Zone will draw on the tranche the company originally announced.\n",
      "TechCrunch has reached out to Amazon for clarification and will update this piece if we hear back.\n",
      "\n",
      "üìå T√ìM T·∫ÆT:\n",
      "\n",
      "Amazon will invest $5.3 billion in an \"AI Zone\" in Saudi Arabia. The AI Zone will include dedicated Amazon Web Services (AWS) servers, networks, and training and certification programs. The project is funded by Saudi Arabia's Public Investment Fund (PIF)\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u th√¥ng tin v√†o file output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "url = \"https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "title_tag = soup.find('h1', class_='article-hero__title')\n",
    "title = title_tag.text.strip() if title_tag else \"Kh√¥ng t√¨m th·∫•y ti√™u ƒë·ªÅ\"\n",
    "\n",
    "content_block = soup.find('div', class_='entry-content')\n",
    "paragraphs = content_block.find_all('p') if content_block else []\n",
    "content = '\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "images = [img['src'] for img in content_block.find_all('img') if img.get('src')] if content_block else []\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "content_trimmed = content[:3000]\n",
    "summary = summarizer(content, max_length=1000, min_length=50, do_sample=False)[0]['summary_text']\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(\"Ti√™u ƒë·ªÅ:\", title)\n",
    "print(\"URL:\", url)\n",
    "print(\"·∫¢nh:\", images if images else \"Kh√¥ng c√≥ ·∫£nh\")\n",
    "print(\"\\nüì∞ N·ªôi dung b√†i vi·∫øt ƒë·∫ßy ƒë·ªß:\\n\")\n",
    "print(content)\n",
    "print(\"\\nüìå T√ìM T·∫ÆT:\\n\")\n",
    "print(summary)\n",
    "\n",
    "with open('output.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['title', 'url', 'summary', 'content', 'images'])\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'summary': summary,\n",
    "        'content': content,\n",
    "        'images': ', '.join(images)\n",
    "    })\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√£ l∆∞u th√¥ng tin v√†o file output.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44302aa1-5bdd-4656-914b-5058f29fcff8",
   "metadata": {},
   "source": [
    "**DEMO: Crawl Data tr√™n nhi·ªÅu web kh√°c nhau + summary b·∫±ng model HuggingFace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff6bd01e-9e32-4f3f-9387-3ad3d4e8ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∞ URL: https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\n",
      "üìå Ti√™u ƒë·ªÅ: AWS enters into ‚Äòstrategic partnership‚Äô with Saudi Arabia-backed Humain\n",
      "üñºÔ∏è ·∫¢nh: Kh√¥ng c√≥ ·∫£nh\n",
      "\n",
      "üìÑ N·ªôi dung:\n",
      " Amazon says it will work with Humain, the AI company recently launched by Saudi Arabia‚Äôs ruler, Mohammed bin Salman, to invest ‚Äú$5 billion-plus‚Äù in a strategic partnership to build an ‚ÄúAI Zone‚Äù in Saudi Arabia.\n",
      "The AI Zone will include dedicated Amazon Web Services (AWS) AI infrastructure, servers, networks, and training and certification programs, according to a press release. Humain is pledging to develop AI solutions using AWS technologies and to work with AWS on providing access to tools and programs for Saudi Arabia-based AI startups.\n",
      "AWS joins tech giants Nvidia, AMD, and others in partnering with Humain, which is funded by Saudi Arabia‚Äôs Public Investment Fund (PIF). American tech firms have looked to the PIF as a source of capital. Companies like¬†Google¬†and¬†Salesforce¬†have also recently worked with the PIF on AI-related projects and investments.\n",
      "President Donald Trump¬†and several tech industry allies attended a U.S.-Saudi investment forum on Tuesday. Under a new Trump administration initiative, U.S.-based tech suppliers, including Nvidia and AMD, have been permitted to arrange deals with Saudi Arabian firms.\n",
      "Saudi Arabia has mandated AI companies and services in the kingdom store data locally, pushing vendors to put facilities there to avoid losing contracts. Both Google and Oracle have announced expansion plans in the region over the past year.\n",
      "Amazon early last March pledged to spend billions of dollars on data centers in Saudi Arabia. On Tuesday, the company said it is devoting around $5.3 billion to develop an AWS region in the kingdom scheduled to come online in 2026. The AI Zone commitments are an ‚Äúadditional investment‚Äù separate from the roughly $5.3 billion, Amazon said. It isn‚Äôt clear, however, if Amazon‚Äôs contribution to the AI Zone will draw on the tranche the company originally announced.\n",
      "TechCrunch has reached out to Amazon for clarification and will update this piece if we hear back.\n",
      "\n",
      "üß† T√≥m t·∫Øt:\n",
      " Amazon will invest $5.3 billion in an \"AI Zone\" in Saudi Arabia. The AI Zone will include dedicated Amazon Web Services (AWS) servers, networks, and training and certification programs. The project is funded by Saudi Arabia's Public Investment Fund (PIF)\n",
      "‚ùå L·ªói khi t√≥m t·∫Øt b√†i vi·∫øt: https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\n",
      "index out of range in self\n",
      "\n",
      "üì∞ URL: https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\n",
      "üìå Ti√™u ƒë·ªÅ: This new ChatGPT feature solves the most annoying thing about Deep Research\n",
      "üñºÔ∏è ·∫¢nh: ['https://cdn.mos.cms.futurecdn.net/omaxTNrWKQyyLuwTVPRAPN.png', 'https://cdn.mos.cms.futurecdn.net/mTaiWitAt8o75BmPY3i4xK.jpg', 'https://cdn.mos.cms.futurecdn.net/8c88QN538yScQuSVce2iym.png', 'https://cdn.mos.cms.futurecdn.net/CnekQ75taQKr2A4BwpNaDQ.jpg', 'https://cdn.mos.cms.futurecdn.net/ujNkHwebXsiadvMMwkynqm.jpg', 'https://cdn.mos.cms.futurecdn.net/ZUqyJvbN2AGcagy7UfZxaj.jpg', 'https://cdn.mos.cms.futurecdn.net/7oCubpx6u5GHwH8Z425yT.jpg', 'https://cdn.mos.cms.futurecdn.net/PWyEJ99KP6Ch6MmR5MdX4J.png', 'https://cdn.mos.cms.futurecdn.net/ViVTMrDq9acYWoJMko5hNR.png', 'https://cdn.mos.cms.futurecdn.net/uYAyjuKucQ4QpzamRdJoeR.jpg', 'https://cdn.mos.cms.futurecdn.net/uhaQk4jWreYwsdkT2AmUuA.jpg', 'https://cdn.mos.cms.futurecdn.net/wVydSQNQJReMzxQNudy2w7.jpg', 'https://cdn.mos.cms.futurecdn.net/StpS7u3AZiQPLgwvLKWn9Y.png', 'https://cdn.mos.cms.futurecdn.net/95neZyuatT65wWMbUv7aJC.jpg', 'https://cdn.mos.cms.futurecdn.net/mJuYV4d7XEFfuXuFWWKshe.jpg', 'https://cdn.mos.cms.futurecdn.net/8kJic4Nst3mXSKgyjMqi6D.jpg', 'https://cdn.mos.cms.futurecdn.net/ViVTMrDq9acYWoJMko5hNR.png', 'https://cdn.mos.cms.futurecdn.net/yhgxFyJ8RbUcoXT7hhSkA7.jpg', 'https://cdn.mos.cms.futurecdn.net/DMdWaFA4DJKzrBHtehFmsN.jpg', 'https://cdn.mos.cms.futurecdn.net/dMHyTqL7epBYHbvgkPkg4h.jpg']\n",
      "\n",
      "üìÑ N·ªôi dung:\n",
      " I've spent a lot of time experimenting with ChatGPT‚Äôs Deep Research feature, and I've produced all kinds of strange (though comprehensive) reports. There's always been a notable gap in its functionality, though, until now. OpenAI has augmented the Deep Research feature with the ability to export your reports as fully formatted PDFs. No more ChatGPT links or screenshots necessary to share what I've learned about the Lake George monster.\n",
      "It's a small interface upgrade, but one that feels like it should have been built into Deep Research from the beginning. Here‚Äôs how it works. You make your Deep Research report or pull up one from a while ago, then click on the share icon at the top of the page. You'll see that the usual 'share link' button now has a companion 'download as PDF' button. One click and your report will be a fully formed, citation-rich PDF in your downloads folder.\n",
      "This export option isn't universally available at the moment. You'll need a subscription to ChatGPT Plus, Team, or Pro. Enterprise and Education users don‚Äôt have it yet, but OpenAI said it‚Äôs coming soon. That's good, as students and professionals are among those I would bet would use Deep Research the most.\n",
      "You can now export your deep research reports as well-formatted PDFs‚Äîcomplete with tables, images, linked citations, and sources.Just click the share icon and select 'Download as PDF.' It works for both new and past reports. pic.twitter.com/kecIR4tEneMay 12, 2025\n",
      "With downloadable PDFs, you can finally do all the things you‚Äôd expect to do with your research. That might mean putting it with other research projects, sharing it with teammates, or just attaching it to an email as part of a bet you're going to win.\n",
      "So yes, this is just a PDF button. But it‚Äôs a PDF button that fixes what used to be one of ChatGPT‚Äôs more frustrating aspects. Now, with downloadable PDFs, you can finally do all the things you‚Äôd expect to do with your research: archive it, share it with teammates, attach it to an email, or even ‚Äì this is my new favorite ‚Äì upload it to another AI.\n",
      "Yes, really. With the PDF in hand, I popped it into Gemini‚Äôs NotebookLM, Google‚Äôs own experimental research assistant. Suddenly, the AI was summarizing my Deep Research report, making flashcards, and suggesting related reading. Then I tried uploading the same PDF into a podcast tool and got an AI-generated episode script out of it. Which means, in a roundabout way, ChatGPT just became a content pipeline. One that exports research and lets other tools remix it into whatever format you need.\n",
      "And that‚Äôs a huge deal.\n",
      "Sign up for breaking news, reviews, opinion, top tech deals, and more.\n",
      "Because the more AI tools we use, the more we‚Äôre going to need bridges between them. OpenAI doesn‚Äôt need to be the everything app, but it does need to be interoperable. Giving users a PDF option is low-hanging fruit, sure, but it‚Äôs also the kind of fruit that lets you bake an entirely new pie. It makes Deep Research portable. It gives it legs. It means I don‚Äôt have to keep 14 tabs open just to reference a well-organized write-up on the history of Japanese vending machines.\n",
      "Of course, OpenAI‚Äôs implementation still has quirks. It‚Äôs a little confusing that the ‚ÄúDownload as PDF‚Äù option isn‚Äôt in the main chat share menu. Most people will assume it‚Äôs not there unless they know where to click. And for a company whose whole pitch is about reducing friction and increasing clarity, burying this behind a second share icon feels oddly off-brand. Still, I‚Äôll take ‚Äúslightly hidden but fully functional‚Äù over ‚Äúcompletely missing‚Äù any day.\n",
      "More importantly, this change signals something else: OpenAI is listening. Maybe not always quickly. Maybe not always intuitively. But enough people have clearly asked for this (or screamed about it on Reddit) that it finally happened. And in a product landscape where most updates feel like AI models arguing over who‚Äôs better at summarizing Aristotle, it‚Äôs refreshing to get a feature that solves a real-world problem.\n",
      "Eric Hal Schwartz is a freelance writer for TechRadar with more than 15 years of experience covering the intersection of the world and technology. For the last five years, he served as head writer for Voicebot.ai and was on the leading edge of reporting on generative AI and large language models. He's since become an expert on the products of generative AI models, such as OpenAI‚Äôs ChatGPT, Anthropic‚Äôs Claude, Google Gemini, and every other synthetic media tool. His experience runs the gamut of media, including print, digital, broadcast, and live events. Now, he's continuing to tell the stories people want and need to hear about the rapidly evolving AI space and its impact on their lives. Eric is based in New York City.\n",
      "Please logout and then login again, you will then be prompted to enter your display name.\n",
      "\n",
      "üß† T√≥m t·∫Øt:\n",
      " ChatGPT's Deep Research feature now lets you export your reports as PDFs. It's a small interface upgrade, but one that feels like it should have been built into Deep Research from the beginning. You'll need a subscription to ChatGPT Plus, Team, or Pro to use it yet.\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u v√†o file articles.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "with open(\"parsers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS = json.load(f)\n",
    "\n",
    "def get_parser_by_domain(domain):\n",
    "    for name, config in PARSERS.items():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def extract_by_config(soup, config):\n",
    "    title_tag = soup.select_one(config[\"title\"])\n",
    "    paragraphs = soup.select(config[\"content\"])\n",
    "    images = [img[\"src\"] for img in soup.select(config[\"images\"]) if img.get(\"src\")]\n",
    "\n",
    "    title = title_tag.text.strip() if title_tag else None\n",
    "    content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"images\": images\n",
    "    }\n",
    "\n",
    "def process_article(url, summarizer):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    domain = urlparse(url).netloc\n",
    "    config = get_parser_by_domain(domain)\n",
    "\n",
    "    if not config:\n",
    "        print(f\"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}\")\n",
    "        return None\n",
    "\n",
    "    article = extract_by_config(soup, config)\n",
    "\n",
    "    if not article[\"title\"] or not article[\"content\"].strip():\n",
    "        print(f\"‚ö†Ô∏è B·ªè qua v√¨ thi·∫øu ti√™u ƒë·ªÅ ho·∫∑c n·ªôi dung: {url}\")\n",
    "        return None\n",
    "\n",
    "    content_trimmed = article[\"content\"][:3000].strip()\n",
    "\n",
    "    try:\n",
    "        summary = summarizer(content_trimmed, max_length=200, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi t√≥m t·∫Øt b√†i vi·∫øt: {url}\\n{e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüì∞ URL:\", url)\n",
    "    print(\"üìå Ti√™u ƒë·ªÅ:\", article[\"title\"])\n",
    "    print(\"üñºÔ∏è ·∫¢nh:\", article[\"images\"] if article[\"images\"] else \"Kh√¥ng c√≥ ·∫£nh\")\n",
    "    print(\"\\nüìÑ N·ªôi dung:\\n\", article[\"content\"])\n",
    "    print(\"\\nüß† T√≥m t·∫Øt:\\n\", summary)\n",
    "\n",
    "    return {\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": url,\n",
    "        \"summary\": summary,\n",
    "        \"content\": article[\"content\"],\n",
    "        \"images\": \", \".join(article[\"images\"])\n",
    "    }\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "urls = [\n",
    "    \"https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\",\n",
    "    \"https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\",\n",
    "    \"https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for url in urls:\n",
    "    result = process_article(url, summarizer)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√£ l∆∞u v√†o file articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46034d8-9339-430c-8b15-495a04b05eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parser_config = {\n",
    "    \"techcrunch\": {\n",
    "        \"domain\": \"techcrunch.com\",\n",
    "        \"title\": \"h1.article-hero__title\",\n",
    "        \"content\": \"div.entry-content p\",\n",
    "        \"images\": \"div.entry-content img\"\n",
    "    },\n",
    "    \"vnexpress\": {\n",
    "        \"domain\": \"vnexpress.net\",\n",
    "        \"title\": \"h1.title-detail\",\n",
    "        \"content\": \"article.fck_detail p\",\n",
    "        \"images\": \"article.fck_detail img\"\n",
    "    },\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"parsers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parser_config, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be6970-b492-4aec-b659-f6bffa1bd549",
   "metadata": {},
   "source": [
    "**Demo: Crawl Data v√† summary b·∫±ng API Gemini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aad9680d-9fb6-4170-be45-c29440ea0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∞ URL: https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\n",
      "üìå Ti√™u ƒë·ªÅ: H∆°n 150 tri·ªáu ƒë·ªìng cho c√°c startup tranh t√†i t·∫°i PitchFest 2025\n",
      "üß† T√≥m t·∫Øt:\n",
      "Cu·ªôc thi PitchFest, ho·∫°t ƒë·ªông tr·ªçng t√¢m c·ªßa Tu·∫ßn l·ªÖ Blockchain & AI Super Vietnam 2025 (4-5/6 t·∫°i ƒê√† N·∫µng), t√¨m ki·∫øm c√°c s√°ng ki·∫øn Web3, blockchain v√† AI. T·ªïng gi·∫£i th∆∞·ªüng ti·ªÅn m·∫∑t 50 tri·ªáu ƒë·ªìng chia cho Top 5, k√®m g√≥i truy·ªÅn th√¥ng 100 tri·ªáu ƒë·ªìng v√† c∆° h·ªôi ti·∫øp c·∫≠n h∆°n 30 qu·ªπ ƒë·∫ßu t∆∞. Cu·ªôc thi g·ªìm 3 v√≤ng: ƒëƒÉng k√Ω, b√¨nh ch·ªçn (20-27/5 - Top 10 v√†o chung k·∫øt) v√† chung k·∫øt (4/6 - pitching 10 ph√∫t). ƒêi·ªÅu ki·ªán tham gia: c√≥ MVP thu·ªôc Web3/AI, tƒÉng tr∆∞·ªüng ng∆∞·ªùi d√πng, ƒë·ªôi ng≈© kinh nghi·ªám, v·ªën g·ªçi d∆∞·ªõi 2 tri·ªáu USD. Super Vietnam 2025 do Orochi Network, FPT Online v√† DSAC t·ªï ch·ª©c, VnExpress b·∫£o tr·ª£, d·ª± ki·∫øn thu h√∫t 7.000 kh√°ch, bao g·ªìm h·ªôi ngh·ªã, tri·ªÉn l√£m, k·∫øt n·ªëi giao th∆∞∆°ng v√† c√°c ho·∫°t ƒë·ªông kh√°c. Orochi Network, ƒë∆°n v·ªã ph√°t tri·ªÉn h·∫° t·∫ßng ki·ªÉm ch·ª©ng d·ªØ li·ªáu, c√≥ kinh nghi·ªám trong lƒ©nh v·ª±c an ninh m·∫°ng v√† h·ª£p t√°c v·ªõi nhi·ªÅu d·ª± √°n Web3.\n",
      "\n",
      "\n",
      "\n",
      "üì∞ URL: https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\n",
      "üìå Ti√™u ƒë·ªÅ: This new ChatGPT feature solves the most annoying thing about Deep Research\n",
      "üß† T√≥m t·∫Øt:\n",
      "ChatGPT Plus, Team, v√† Pro hi·ªán ƒë√£ c√≥ th√™m t√≠nh nƒÉng m·ªõi: xu·∫•t b√°o c√°o Deep Research th√†nh file PDF. Tr∆∞·ªõc ƒë√¢y, ng∆∞·ªùi d√πng ch·ªâ c√≥ th·ªÉ chia s·∫ª b√°o c√°o qua link ho·∫∑c ch·ª•p m√†n h√¨nh. Gi·ªù ƒë√¢y, ch·ªâ c·∫ßn m·ªôt c√∫ nh·∫•p chu·ªôt v√†o n√∫t \"download as PDF\" trong m·ª•c chia s·∫ª, ng∆∞·ªùi d√πng c√≥ th·ªÉ t·∫£i xu·ªëng m·ªôt file PDF ho√†n ch·ªânh, ƒë·∫ßy ƒë·ªß tr√≠ch d·∫´n. T√≠nh nƒÉng n√†y gi√∫p ng∆∞·ªùi d√πng d·ªÖ d√†ng l∆∞u tr·ªØ, chia s·∫ª v√† s·ª≠ d·ª•ng b√°o c√°o cho nhi·ªÅu m·ª•c ƒë√≠ch kh√°c nhau, th·∫≠m ch√≠ l√† ƒë∆∞a v√†o c√°c c√¥ng c·ª• AI kh√°c ƒë·ªÉ t·∫°o ra n·ªôi dung m·ªõi. M·∫∑c d√π c√≤n m·ªôt v√†i ƒëi·ªÉm c·∫ßn c·∫£i thi·ªán v·ªÅ giao di·ªán, vi·ªác OpenAI l·∫Øng nghe v√† b·ªï sung t√≠nh nƒÉng n√†y cho th·∫•y s·ª± quan t√¢m ƒë·∫øn nhu c·∫ßu th·ª±c t·∫ø c·ªßa ng∆∞·ªùi d√πng.\n",
      "\n",
      "\n",
      "\n",
      "üì∞ URL: https://techcrunch.com/2025/05/13/attend-techcrunch-sessions-ai-with-this-new-limited-time-discount/\n",
      "üìå Ti√™u ƒë·ªÅ: Attend TechCrunch Sessions: AI with this new, limited-time discount\n",
      "üß† T√≥m t·∫Øt:\n",
      "TechCrunch Sessions: AI gi·∫£m gi√° v√© tham d·ª± s·ª± ki·ªán AI t·∫°i UC Berkeley v√†o ng√†y 5 th√°ng 6, ch·ªâ c√≤n $292/v√© v√† gi·∫£m 50% cho v√© th·ª© hai. S·ª± ki·ªán n√†y d√†nh cho t·∫•t c·∫£ m·ªçi ng∆∞·ªùi quan t√¢m ƒë·∫øn AI, bao g·ªìm c√°c chuy√™n gia, nh√† s√°ng l·∫≠p, h·ªçc gi·∫£, v√† ng∆∞·ªùi ƒëam m√™ AI. Ng∆∞·ªùi tham gia s·∫Ω c√≥ c∆° h·ªôi h·ªçc h·ªèi, k·∫øt n·ªëi, tham gia c√°c bu·ªïi networking 1:1 v√† c√°c s·ª± ki·ªán b√™n l·ªÅ do c√°c ƒë·ªëi t√°c t·ªï ch·ª©c. ƒê√¢y l√† ∆∞u ƒë√£i c√≥ gi·ªõi h·∫°n, h√£y nhanh tay ƒëƒÉng k√Ω ƒë·ªÉ kh√¥ng b·ªè l·ª°.\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u xong v√†o file articles.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = \"AIzaSyA6vrOzFkbeSgn0Ixh2EItOOwCxKFsQqgU\"\n",
    "\n",
    "def summarize_with_gemini(content):\n",
    "    prompt = (\n",
    "        \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t√≥m t·∫Øt tin t·ª©c. \"\n",
    "        \"H√£y ƒë·ªçc ƒëo·∫°n n·ªôi dung sau v√† t√≥m t·∫Øt l·∫°i ƒë·∫ßy ƒë·ªß, r√µ r√†ng c√°c √Ω ch√≠nh\"\n",
    "        \"H√£y vi·∫øt d·ªÖ hi·ªÉu, s√∫c t√≠ch v√† gi·ªØ ƒë√∫ng tinh th·∫ßn b√†i vi·∫øt.\\n\\n\"\n",
    "        f\"{content.strip()}\"\n",
    "    )\n",
    "\n",
    "    body = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\"text\": prompt}]\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    # url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=AIzaSyA6vrOzFkbeSgn0Ixh2EItOOwCxKFsQqgU\"\n",
    "    url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyA6vrOzFkbeSgn0Ixh2EItOOwCxKFsQqgU\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "with open(\"parsers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS = json.load(f)\n",
    "\n",
    "def get_parser_by_domain(domain):\n",
    "    for config in PARSERS.values():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def extract_by_config(soup, config):\n",
    "    title_tag = soup.select_one(config[\"title\"])\n",
    "    content_tags = soup.select(config[\"content\"])\n",
    "    image_tags = soup.select(config[\"images\"])\n",
    "\n",
    "    title = title_tag.text.strip() if title_tag else None\n",
    "    content = \"\\n\".join(p.text.strip() for p in content_tags if p.text.strip())\n",
    "    images = [img[\"src\"] for img in image_tags if img.get(\"src\")]\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"images\": images\n",
    "    }\n",
    "\n",
    "def process_article(url):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    domain = urlparse(url).netloc\n",
    "    config = get_parser_by_domain(domain)\n",
    "\n",
    "    if not config:\n",
    "        print(f\"‚ùå Kh√¥ng c√≥ parser cho domain: {domain}\")\n",
    "        return None\n",
    "\n",
    "    article = extract_by_config(soup, config)\n",
    "    if not article[\"title\"] or not article[\"content\"]:\n",
    "        print(f\"‚ö†Ô∏è B·ªè qua v√¨ thi·∫øu ti√™u ƒë·ªÅ ho·∫∑c n·ªôi dung: {url}\")\n",
    "        return None\n",
    "\n",
    "    summary = summarize_with_gemini(article[\"content\"][:5000])\n",
    "    if not summary:\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nüì∞ URL: {url}\")\n",
    "    print(f\"üìå Ti√™u ƒë·ªÅ: {article['title']}\")\n",
    "    # print(f\"N·ªôi dung:{article['content']}\")\n",
    "    print(f\"üß† T√≥m t·∫Øt:\\n{summary}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": url,\n",
    "        \"summary\": summary,\n",
    "        \"content\": article[\"content\"],\n",
    "        \"images\": \", \".join(article[\"images\"])\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        \"https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\",\n",
    "        \"https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\",\n",
    "        \"https://techcrunch.com/2025/05/13/attend-techcrunch-sessions-ai-with-this-new-limited-time-discount/\"\n",
    "    ]\n",
    "    urls1 = [\"https://techcrunch.com/2025/05/13/anthropic-google-score-win-by-nabbing-openai-backed-harvey-as-a-user/\"]\n",
    "\n",
    "    results = []\n",
    "    for url in urls:\n",
    "        result = process_article(url)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"\\n‚úÖ ƒê√£ l∆∞u xong v√†o file articles.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl_kernel",
   "language": "python",
   "name": "new_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
