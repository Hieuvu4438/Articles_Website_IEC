import json
import time
import requests
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from datetime import datetime

class NewsCrawler:
    def __init__(self):
        """Kh·ªüi t·∫°o News Crawler"""
        self.GEMINI_API_KEY = ""
        self.crawled_urls = set()  # L∆∞u danh s√°ch URL ƒë√£ crawl ƒë·ªÉ tr√°nh duplicate
        self.load_configs()
        
    def load_configs(self):
        """Load c·∫•u h√¨nh parser t·ª´ dictionary trong code"""
        self.PARSERS = {
            "techcrunch": {
                "domain": "techcrunch.com",
                "title": "div.article-hero__middle",
                "content": "div.entry-content",
                "images": "div.entry-content img",
                "author": "a.wp-block-tc23-author-card-name__link",
                "time": "time",
                "highlight": "a nofollow",
                "topic": "div.tc23-post-relevant-terms__terms a",
                "references": "div.entry-content a"
            },
            "vnexpress": {
                "domain": "vnexpress.net",
                "title": "h1.title-detail",
                "content": "article.fck_detail p.Normal",
                "images": "article.fck_detail img",
                "author": "article.fck_detail p.Normal strong",
                "time": "div.sidebar-1 span.date",
                "highlight": "p.description",
                "topic": "ul.breadcrumb a",
                "references": "div.width_common.box-tinlienquanv2 a"
            },
            "techradar": {
                "domain": "techradar.com",
                "title": "div.news-article header h1",
                "content": "div.wcp-item-content p",
                "images": "div.wcp-item-content img",
                "author": "",
                "time": "",
                "highlight": "",
                "topic": "",
                "references": ""
            },
            "vietnamnet": {
                "domain": "https://vietnamnet.vn",
                "title": "div.news-article header h1",
                "content": "div.wcp-item-content p",
                "images": "div.wcp-item-content img",
                "author": "",
                "time": "",
                "highlight": "",
                "topic": "",
                "references": ""
            }
        }

        self.PARSERS_LINKS = {
            "techcrunch": {
                "domain": "techcrunch.com",
                "links": "a.loop-card__title-link"
            },
            "vnexpress": {
                "domain": "vnexpress.net",
                "links": ".wrapper-topstory-folder a[href*='vnexpress.net']"
            }
        }
            
        # Topics categories t·ª´ file JSON
        self.TOPICS_CATEGORIES = {
            "categories": {
                "Tr√≠ tu·ªá nh√¢n t·∫°o v√† h·ªçc m√°y": [
                    "AI trong t√†i ch√≠nh ng√¢n h√†ng",
                    "AI t·∫°o sinh",
                    "AI ƒë·∫°o ƒë·ª©c v√† quy ƒë·ªãnh",
                    "AI trong ph√°t tri·ªÉn ph·∫ßn m·ªÅm"
                ],
                "Ph√°t tri·ªÉn ph·∫ßn m·ªÅm v√† d·ªãch v·ª• IT": [
                    "Ph√°t tri·ªÉn ph·∫ßn m·ªÅm di ƒë·ªông / Web",
                    "D·ªãch v·ª• Blockchain",
                    "Ph√¢n t√≠ch d·ªØ li·ªáu l·ªõn",
                    "ƒêi·ªán to√°n ƒë√°m m√¢y",
                    "Xu h∆∞·ªõng gia c√¥ng ph·∫ßn m·ªÅm"
                ],
                "Ph·∫ßn c·ª©ng v√† ƒëi·ªán t·ª≠ ti√™u d√πng": [
                    "Thi·∫øt b·ªã ƒëi·ªán t·ª≠",
                    "Thi·∫øt b·ªã √¢m thanh",
                    "ƒêi·ªán t·ª≠ gia d·ª•ng",
                    "ƒê√°nh gi√° ph·∫ßn c·ª©ng v√† th·ªß thu·∫≠t",
                    "Ph·∫ßn c·ª©ng ch∆°i game"
                ],
                "An ninh m·∫°ng": [
                    "B·∫£o v·ªá d·ªØ li·ªáu",
                    "An ninh internet",
                    "Ch√≠nh s√°ch an ninh m·∫°ng",
                    "Ph√≤ng ch·ªëng t·∫•n c√¥ng m·∫°ng",
                    "Gi·∫£i ph√°p an ninh doanh nghi·ªáp"
                ],
                "Fintech v√† th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠": [
                    "Fintech",
                    "N·ªÅn t·∫£ng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠",
                    "Ng√¢n h√†ng k·ªπ thu·∫≠t s·ªë",
                    "H·ªá th·ªëng thanh to√°n",
                    "ƒê·∫ßu t∆∞"
                ],
                "C√¥ng ngh·ªá m·ªõi": [
                    "IoT",
                    "VR / AR",
                    "Th∆∞∆°ng m·∫°i h√≥a 5G",
                    "B√°n d·∫´n",
                    "Chuy·ªÉn ƒë·ªïi s·ªë"
                ],
                "Ch√≠nh s√°ch c√¥ng ngh·ªá, kinh doanh v√† xu h∆∞·ªõng": [
                    "ƒê·∫ßu t∆∞ c√¥ng ngh·ªá / Startup",
                    "Quy·ªÅn s·ªü h·ªØu tr√≠ tu·ªá",
                    "Giao thoa khoa h·ªçc - c√¥ng ngh·ªá",
                    "S·ª± ki·ªán c√¥ng ngh·ªá to√†n c·∫ßu"
                ]
            }
        }

    def is_url_crawled(self, url):
        """Ki·ªÉm tra URL ƒë√£ ƒë∆∞·ª£c crawl ch∆∞a"""
        return url in self.crawled_urls

    def mark_url_as_crawled(self, url):
        """ƒê√°nh d·∫•u URL ƒë√£ ƒë∆∞·ª£c crawl"""
        self.crawled_urls.add(url)

    def get_crawled_count(self):
        """L·∫•y s·ªë l∆∞·ª£ng URL ƒë√£ crawl"""
        return len(self.crawled_urls)

    def reset_crawled_urls(self):
        """Reset danh s√°ch URL ƒë√£ crawl (d√πng khi c·∫ßn crawl l·∫°i t·ª´ ƒë·∫ßu)"""
        self.crawled_urls.clear()

    def get_parser_by_domain(self, domain):
        """L·∫•y c·∫•u h√¨nh parser theo domain"""
        for config in self.PARSERS.values():
            if config["domain"] in domain:
                return config
        return None

    def get_parser_link(self, domain):
        """L·∫•y c·∫•u h√¨nh parser links theo domain"""
        for config in self.PARSERS_LINKS.values():
            if config["domain"] in domain:
                return config
        return None

    def get_latest_news(self, soup, config):
        """L·∫•y danh s√°ch links tin t·ª©c m·ªõi nh·∫•t"""
        links_tags = soup.select(config["links"])
        links = [a['href'] for a in links_tags]
        return links

    def summarize_with_gemini(self, content):
        """T√≥m t·∫Øt n·ªôi dung s·ª≠ d·ª•ng Gemini AI"""
        prompt = (
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t√≥m t·∫Øt tin t·ª©c v·ªõi ƒë·ªô ch√≠nh x√°c cao. "
            "Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc ƒëo·∫°n n·ªôi dung sau v√† t√≥m t·∫Øt c√°c √Ω ch√≠nh m·ªôt c√°ch s√∫c t√≠ch, d·ªÖ hi·ªÉu, gi·ªØ nguy√™n tinh th·∫ßn v√† th√¥ng tin quan tr·ªçng c·ªßa b√†i vi·∫øt. "
            "T√≥m t·∫Øt ng·∫Øn g·ªçn, s·ª≠ d·ª•ng ng√¥n ng·ªØ t·ª± nhi√™n, trung l·∫≠p v√† kh√¥ng th√™m th√¥ng tin ngo√†i n·ªôi dung g·ªëc. "
            "Tr√¨nh b√†y t√≥m t·∫Øt theo ƒëo·∫°n vƒÉn b·∫£n, kh√¥ng s·ª≠ d·ª•ng markdown, kh√¥ng d√πng k√≠ hi·ªáu ƒë·∫∑c bi·ªát, kh√¥ng c·∫ßn l·ªùi d·∫´n, ch·ªâ c·∫ßn tr·ªçng t√¢m n·ªôi dung"
            "Tr√°nh l·∫∑p l·∫°i t·ª´ ng·ªØ kh√¥ng c·∫ßn thi·∫øt v√† ƒë·∫£m b·∫£o kh√¥ng b·ªè s√≥t th√¥ng tin c·ªët l√µi.\n\n"
            f"{content.strip()}"
        )
  
        body = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }

        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=" + self.GEMINI_API_KEY
        headers = {"Content-Type": "application/json"}

        try:
            response = requests.post(url, headers=headers, json=body)
            response.raise_for_status()
            result = response.json()
            return result["candidates"][0]["content"]["parts"][0]["text"]
        except Exception as e:
            return None

    def classify_topics_with_gemini(self, title, content, summary, original_topics):
        """Ph√¢n lo·∫°i topics s·ª≠ d·ª•ng Gemini AI v·ªõi danh s√°ch topics c·ªë ƒë·ªãnh"""
        # T·∫°o c·∫•u tr√∫c categories cho prompt
        categories_text = ""
        all_subcategories = []
        
        for main_category, subcategories in self.TOPICS_CATEGORIES["categories"].items():
            categories_text += f"\nüìÇ {main_category}:\n"
            for sub in subcategories:
                all_subcategories.append(sub)
                categories_text += f"   ‚Ä¢ {sub}\n"

        prompt = (
            "B·∫°n l√† chuy√™n gia ph√¢n lo·∫°i tin t·ª©c c√¥ng ngh·ªá. Nhi·ªám v·ª•: ph√¢n t√≠ch b√†i vi·∫øt v√† ch·ªçn topics ph√π h·ª£p t·ª´ danh s√°ch c√≥ s·∫µn.\n\n"
            
            "=== TH√îNG TIN B√ÄI VI·∫æT ===\n"
            f"Ti√™u ƒë·ªÅ: {title}\n"
            f"N·ªôi dung: {content[:300]}{'...' if len(content) > 300 else ''}\n"
            f"T√≥m t·∫Øt: {summary}\n"
            f"Topics g·ªëc: {', '.join(original_topics) if original_topics else 'Kh√¥ng c√≥'}\n\n"
            
            "=== DANH S√ÅCH TOPICS C√ì S·∫¥N ===\n"
            f"{categories_text}\n"
            
            "=== H∆Ø·ªöNG D·∫™N ===\n"
            "1. ƒê·ªçc k·ªπ n·ªôi dung b√†i vi·∫øt\n"
            "2. Ch·ªçn 1-3 topics con ph√π h·ª£p nh·∫•t t·ª´ danh s√°ch tr√™n\n"
            "3. CH·ªà s·ª≠ d·ª•ng t√™n topics CH√çNH X√ÅC nh∆∞ trong danh s√°ch\n"
            "4. Tr·∫£ v·ªÅ JSON format:\n\n"
            
            '{"selected_topics": ["topic1", "topic2"]}\n\n'
            
            "V√ç D·ª§:\n"
            '{"selected_topics": ["AI t·∫°o sinh", "Ph√°t tri·ªÉn ph·∫ßn m·ªÅm di ƒë·ªông / Web"]}\n\n'
            
            "QUAN TR·ªåNG: CH·ªà tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch g√¨ th√™m!"
        )

        body = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }

        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=" + self.GEMINI_API_KEY
        headers = {"Content-Type": "application/json"}

        try:
            response = requests.post(url, headers=headers, json=body)
            response.raise_for_status()
            result = response.json()
            response_text = result["candidates"][0]["content"]["parts"][0]["text"]
            
            # Debug: in response ƒë·ªÉ ki·ªÉm tra
            print(f"üîç DEBUG - Gemini Response: {response_text[:200]}...")
            
            # Parse JSON response v·ªõi format ƒë∆°n gi·∫£n
            try:
                import re
                # T√¨m JSON object trong response  
                json_match = re.search(r'\{[^{}]*"selected_topics"[^{}]*\}', response_text)
                if json_match:
                    print(f"üîç DEBUG - Found JSON: {json_match.group()}")
                    json_data = json.loads(json_match.group())
                    selected_topics = json_data.get("selected_topics", [])
                    
                    # Validate topics - ch·ªâ gi·ªØ l·∫°i topics c√≥ trong danh s√°ch
                    valid_topics = []
                    all_valid_subcategories = []
                    
                    # T·∫°o danh s√°ch t·∫•t c·∫£ subcategories h·ª£p l·ªá
                    for main_cat, sub_cats in self.TOPICS_CATEGORIES["categories"].items():
                        all_valid_subcategories.extend(sub_cats)
                    
                    for topic in selected_topics:
                        if topic in all_valid_subcategories:
                            valid_topics.append(topic)
                    
                    print(f"üîç DEBUG - Valid topics: {valid_topics}")
                    return {"selected_topics": valid_topics}
                else:
                    print("‚ùå DEBUG - No JSON found in response")
                    return {}
            except json.JSONDecodeError as e:
                print(f"‚ùå DEBUG - JSON decode error: {e}")
                return {}
                
        except Exception as e:
            print(f"‚ùå DEBUG - Request error: {e}")
            return {}

    def summarize(self, soup, config):
        """T√≥m t·∫Øt b√†i vi·∫øt v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin"""
        try:
            title_tag = soup.select_one(config["title"])
            content_tags = soup.select(config["content"])
            image_tags = soup.select(config["images"])
            author_tag = soup.select_one(config["author"])
            time_tag = soup.select_one(config["time"])
            topics_tag = soup.select(config["topic"])
            references_tags = soup.select(config["references"])
            
            title = title_tag.text.strip() if title_tag else None
            content = "\n".join(p.text.strip() for p in content_tags if p.text.strip())
            summ = self.summarize_with_gemini(content)
            images = [img["src"] for img in image_tags if img.get("src")]
            author = author_tag.text.strip() if author_tag else None
            time = time_tag.text.strip() if time_tag else None
            topics = [topic.get_text(strip=True) for topic in topics_tag]
            references = [link['href'] for link in references_tags if link.get('href')]
            
            return {
                "title": title,
                "content": content,
                "images": images,
                "author": author,
                "time": time,
                "topic": topics,
                "references": references,
                "summary": summ,
            }
        except Exception as e:
            return None

    def parser_seo(self, soup, config):
        """Ph√¢n t√≠ch d·ªØ li·ªáu SEO c·ªßa b√†i vi·∫øt"""
        try:
            seo_data = {
                "meta_title": "",
                "meta_description": "",
                "meta_keywords": [],
                "h1": "",
                "h2": [],
                "canonical_url": "",
                "word_count": 0,
                "internal_links": [],
                "external_links": []
            }
            
            meta_title = soup.find("meta", property="og:title") or soup.find("meta", attrs={"name": "title"})
            if meta_title and meta_title.get("content"):
                seo_data["meta_title"] = meta_title["content"]
                
            meta_desc = soup.find("meta", property="og:description") or soup.find("meta", attrs={"name": "description"})
            if meta_desc and meta_desc.get("content"):
                seo_data["meta_description"] = meta_desc["content"]
                
            meta_keywords = soup.find("meta", attrs={"name": "keywords"})
            if meta_keywords and meta_keywords.get("content"):
                seo_data["meta_keywords"] = [kw.strip() for kw in meta_keywords["content"].split(",")]
                
            h1_tag = soup.find("h1")
            if h1_tag:
                seo_data["h1"] = h1_tag.get_text(strip=True)
                
            h2_tags = soup.find_all("h2")
            if h2_tags:
                seo_data["h2"] = [h2.get_text(strip=True) for h2 in h2_tags]
                
            canonical = soup.find("link", rel="canonical")
            if canonical and canonical.get("href"):
                seo_data["canonical_url"] = canonical["href"]
                
            content_container = soup.select_one(config.get("content", ""))
            if content_container:
                text_content = " ".join(p.get_text(strip=True) for p in content_container.find_all("p"))
                seo_data["word_count"] = len(text_content.split())

            domain = config["domain"]
            links = soup.select(config.get("references", ""))
            for link in links:
                href = link.get("href")
                if href:
                    if domain in href or href.startswith("/"):
                        seo_data["internal_links"].append(href)
                    else:
                        seo_data["external_links"].append(href)

            return seo_data
        except Exception as e:
            return None

    def create_driver(self):
        """T·∫°o Chrome driver"""
        options = Options()
        options.headless = True
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        return driver

    def process_article(self, url):
        """X·ª≠ l√Ω m·ªôt b√†i vi·∫øt t·ª´ URL v√† return dict t·ªïng h·ª£p"""
        # Ki·ªÉm tra URL ƒë√£ ƒë∆∞·ª£c crawl ch∆∞a
        if self.is_url_crawled(url):
            return None
            
        try:
            driver = self.create_driver()
            driver.get(url)
            time.sleep(3)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            driver.quit()
            
            domain = urlparse(url).netloc 
            config = self.get_parser_by_domain(domain)
            
            if not config:
                return None

            # Thu th·∫≠p d·ªØ li·ªáu
            summarize_article = self.summarize(soup, config)
            seo_data = self.parser_seo(soup, config)
            
            if not summarize_article or not seo_data:
                return None
            
            # G·ªôp d·ªØ li·ªáu t·ª´ articles_summarize v√† seo_data
            combined_data = {
                # D·ªØ li·ªáu t·ª´ articles_summarize
                "title": summarize_article.get("title", ""),
                "content": summarize_article.get("content", ""),
                "images": summarize_article.get("images", []),
                "author": summarize_article.get("author", ""),
                "time": summarize_article.get("time", ""),
                "topic": summarize_article.get("topic", []),
                "references": summarize_article.get("references", []),
                "summary": summarize_article.get("summary", ""),
                
                # D·ªØ li·ªáu t·ª´ seo_data
                "meta_title": seo_data.get("meta_title", ""),
                "meta_description": seo_data.get("meta_description", ""),
                "meta_keywords": seo_data.get("meta_keywords", []),
                "h1": seo_data.get("h1", ""),
                "h2": seo_data.get("h2", []),
                "canonical_url": seo_data.get("canonical_url", ""),
                "word_count": seo_data.get("word_count", 0),
                "internal_links": seo_data.get("internal_links", []),
                "external_links": seo_data.get("external_links", []),
                
                # Th√¥ng tin b·ªï sung
                "url": url,
                "domain": domain,
                "crawl_timestamp": datetime.now().isoformat()
            }
            
            # Th√™m ph√¢n lo·∫°i topics t·ª± ƒë·ªông n·∫øu c√≥ ƒë·ªß d·ªØ li·ªáu
            if combined_data.get("title") and combined_data.get("summary"):
                classified_result = self.classify_topics_with_gemini(
                    combined_data.get("title", ""),
                    combined_data.get("content", ""),
                    combined_data.get("summary", ""),
                    combined_data.get("topic", [])
                )
                combined_data["rcm_topics"] = classified_result
            
            # ƒê√°nh d·∫•u URL ƒë√£ crawl th√†nh c√¥ng
            self.mark_url_as_crawled(url)
            
            return combined_data
            
        except Exception as e:
            return None

    def crawl_multiple_urls(self):
        """Crawl nhi·ªÅu URL t·ª´ c√°c domain ƒë√£ c·∫•u h√¨nh v√† return danh s√°ch dict"""
        try:
            crawled_articles = []
            skipped_count = 0
            processed_count = 0
            domains = [config["domain"] for config in self.PARSERS.values()]
            
            print(f"üìä B·∫Øt ƒë·∫ßu crawl. ƒê√£ c√≥ {self.get_crawled_count()} URL trong cache.")
            
            for domain in domains:
                config_links = self.get_parser_link(domain)
                if not config_links:
                    continue
                    
                driver = self.create_driver()
                driver.get(f"https://{domain}")
                time.sleep(3)
                soup = BeautifulSoup(driver.page_source, "html.parser")
                driver.quit()
                
                links = self.get_latest_news(soup, config_links)
                for url in links[:2]:  # L·∫•y 2 b√†i m·ªõi nh·∫•t
                    if url.startswith("/"):
                        url = f"https://{domain}{url}"

                    processed_count += 1
                    if self.is_url_crawled(url):
                        skipped_count += 1
                        continue
                        
                    result = self.process_article(url)
                    if result:
                        crawled_articles.append(result)
            
            print(f"üìà K·∫øt qu·∫£: {len(crawled_articles)} b√†i m·ªõi, {skipped_count} b√†i ƒë√£ c√≥, {processed_count} b√†i ƒë√£ x·ª≠ l√Ω")
            return crawled_articles
            
        except Exception as e:
            return []

    def auto_crawl_scheduler(self, interval_minutes=5):
        """Ch·ª©c nƒÉng t·ª± ƒë·ªông crawl theo th·ªùi gian"""
        try:
            while True:
                results = self.crawl_multiple_urls()
                
                # Ngh·ªâ theo th·ªùi gian ƒë√£ ƒë·ªãnh
                time.sleep(interval_minutes * 60)
                
        except KeyboardInterrupt:
            pass
        except Exception as e:
            time.sleep(60)  # Ngh·ªâ 1 ph√∫t tr∆∞·ªõc khi th·ª≠ l·∫°i
            self.auto_crawl_scheduler(interval_minutes)

    def crawl_single_url(self, url):
        """Crawl m·ªôt URL ƒë∆°n l·∫ª v√† return dict"""
        result = self.process_article(url)
        return result


def main():
    """H√†m ch√≠nh"""
    crawler = NewsCrawler()
    
    print("ü§ñ News Crawler - H·ªá th·ªëng thu th·∫≠p tin t·ª©c t·ª± ƒë·ªông")
    print("=" * 60)
    print("1. üöÄ Crawl t·∫•t c·∫£ domain") 
    print("2. üîç Crawl m·ªôt URL")
    print("3. ‚öôÔ∏è  Auto crawl (t√πy ch·ªânh th·ªùi gian)")
    print("4. üóëÔ∏è  Reset cache (x√≥a danh s√°ch URL ƒë√£ crawl)")
    print(f"üìä Cache hi·ªán t·∫°i: {crawler.get_crawled_count()} URL")

    choice = input("\nCh·ªçn ch·ª©c nƒÉng (1-4): ").strip()
    if choice == "1":
        print("üöÄ B·∫Øt ƒë·∫ßu crawl t·∫•t c·∫£ domain...")
        results = crawler.crawl_multiple_urls()
        if results:
            print(f"\nüéâ Th√†nh c√¥ng! ƒê√£ crawl {len(results)} b√†i vi·∫øt.")
            print("üìã D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tr·∫£ v·ªÅ trong bi·∫øn results.")
            with open("crawled_data.json", "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            return results
        
    elif choice == "2":
        url = input("Nh·∫≠p URL: ").strip()
        if url:
            print("üîç Crawl URL...")
            result = crawler.crawl_single_url(url)
            if result:
                print("üéâ Th√†nh c√¥ng! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tr·∫£ v·ªÅ.")
                with open("crawled_data_single.json", "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=4)
                return result
        else:
            print("‚ùå URL kh√¥ng h·ª£p l·ªá!")
        
    elif choice == "3":
        try:
            minutes = int(input("Nh·∫≠p s·ªë ph√∫t gi·ªØa m·ªói l·∫ßn crawl (t·ªëi thi·ªÉu 1 ph√∫t): ").strip())
            if minutes < 1:
                print("‚ùå Th·ªùi gian t·ªëi thi·ªÉu l√† 1 ph√∫t!")
                return
            print(f"‚è∞ Ch·∫ø ƒë·ªô auto crawl t√πy ch·ªânh ({minutes} ph√∫t/l·∫ßn)")
            crawler.auto_crawl_scheduler(minutes)
        except ValueError:
            print("‚ùå Vui l√≤ng nh·∫≠p s·ªë nguy√™n h·ª£p l·ªá!")
    
    elif choice == "4":
        old_count = crawler.get_crawled_count()
        crawler.reset_crawled_urls()
        print(f"üóëÔ∏è  ƒê√£ x√≥a {old_count} URL kh·ªèi cache. Cache hi·ªán t·∫°i: {crawler.get_crawled_count()} URL")
            
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")


if __name__ == "__main__":
    main()
