#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple News Crawler - PhiÃªn báº£n test Ä‘Æ¡n giáº£n
Chuyá»ƒn Ä‘á»•i tá»« Jupyter Notebook TEST.ipynb
"""

import csv
import json
import time
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from transformers import pipeline

class SimpleNewsCrawler:
    def __init__(self):
        """Khá»Ÿi táº¡o Simple News Crawler"""
        self.load_parsers()
        self.summarizer = None
        
    def load_parsers(self):
        """Load cáº¥u hÃ¬nh parser"""
        try:
            with open("parsers.json", "r", encoding="utf-8") as f:
                self.PARSERS = json.load(f)
        except FileNotFoundError:
            self.create_default_parsers()
    
    def create_default_parsers(self):
        """Táº¡o parser máº·c Ä‘á»‹nh náº¿u chÆ°a cÃ³ file"""
        self.PARSERS = {
            "techcrunch": {
                "domain": "techcrunch.com",
                "title": "div.article-hero__middle",
                "content": "div.entry-content",
                "images": "div.entry-content img"
            },
            "vnexpress": {
                "domain": "vnexpress.net",
                "title": "h1.title-detail",
                "content": "div.sidebar-1",
                "images": "article.fck_detail img"
            },
            "techradar": {
                "domain": "techradar.com",
                "title": "div.news-article header h1",
                "content": "div.wcp-item-content p",
                "images": "div.wcp-item-content img"
            }
        }
        
        with open("parsers.json", "w", encoding="utf-8") as f:
            json.dump(self.PARSERS, f, ensure_ascii=False, indent=4)
        
        print("âœ… ÄÃ£ táº¡o file parsers.json máº·c Ä‘á»‹nh")

    def get_parser_by_domain(self, domain):
        """Láº¥y parser theo domain"""
        for name, config in self.PARSERS.items():
            if config["domain"] in domain:
                return config
        return None

    def extract_by_config(self, soup, config):
        """TrÃ­ch xuáº¥t ná»™i dung theo config"""
        title_tag = soup.select_one(config["title"])
        paragraphs = soup.select(config["content"])
        images = [img["src"] for img in soup.select(config["images"]) if img.get("src")]

        title = title_tag.text.strip() if title_tag else None
        content = "\n".join(p.text.strip() for p in paragraphs if p.text.strip())

        return {
            "title": title,
            "content": content,
            "images": images
        }

    def init_summarizer(self):
        """Khá»Ÿi táº¡o model tÃ³m táº¯t"""
        if self.summarizer is None:
            print("ğŸ¤– Äang táº£i model tÃ³m táº¯t...")
            try:
                self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
                print("âœ… ÄÃ£ táº£i model tÃ³m táº¯t!")
            except Exception as e:
                print(f"âŒ Lá»—i khi táº£i model: {e}")
                print("ğŸ’¡ CÃ i Ä‘áº·t transformers: pip install transformers torch")
                return False
        return True

    def create_driver(self):
        """Táº¡o Chrome driver"""
        options = Options()
        options.headless = True
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        return driver

    def process_article(self, url, use_summarizer=True):
        """Xá»­ lÃ½ má»™t bÃ i viáº¿t"""
        try:
            driver = self.create_driver()
            driver.get(url)
            time.sleep(3)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            driver.quit()

            domain = urlparse(url).netloc
            config = self.get_parser_by_domain(domain)

            if not config:
                print(f"âŒ KhÃ´ng cÃ³ parser cho domain: {domain}")
                return None

            article = self.extract_by_config(soup, config)

            if not article["title"] or not article["content"].strip():
                print(f"âš ï¸ Bá» qua vÃ¬ thiáº¿u tiÃªu Ä‘á» hoáº·c ná»™i dung: {url}")
                return None

            # TÃ³m táº¯t náº¿u Ä‘Æ°á»£c yÃªu cáº§u
            summary = ""
            if use_summarizer and self.init_summarizer():
                content_trimmed = article["content"][:3000].strip()
                try:
                    summary = self.summarizer(content_trimmed, max_length=200, min_length=50, do_sample=False)[0]["summary_text"]
                except Exception as e:
                    print(f"âŒ Lá»—i khi tÃ³m táº¯t bÃ i viáº¿t: {url}\n{e}")
                    summary = "KhÃ´ng thá»ƒ tÃ³m táº¯t"

            print("\n" + "="*60)
            print("ğŸ“° URL:", url)
            print("ğŸ“Œ TiÃªu Ä‘á»:", article["title"])
            print("ğŸ–¼ï¸ áº¢nh:", article["images"] if article["images"] else "KhÃ´ng cÃ³ áº£nh")
            print("\nğŸ“„ Ná»™i dung:\n", article["content"][:500] + "..." if len(article["content"]) > 500 else article["content"])
            if summary:
                print("\nğŸ§  TÃ³m táº¯t:\n", summary)
            print("="*60)

            return {
                "title": article["title"],
                "url": url,
                "summary": summary,
                "content": article["content"],
                "images": ", ".join(article["images"]) if article["images"] else ""
            }

        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ URL {url}: {e}")
            return None

    def crawl_urls(self, urls, use_summarizer=True):
        """Crawl danh sÃ¡ch URLs"""
        results = []
        
        print(f"ğŸš€ Báº¯t Ä‘áº§u crawl {len(urls)} URL(s)...")
        
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ“ Äang xá»­ lÃ½ {i}/{len(urls)}: {url}")
            result = self.process_article(url, use_summarizer)
            if result:
                results.append(result)
            
            # Nghá»‰ giá»¯a cÃ¡c request
            if i < len(urls):
                time.sleep(2)

        return results

    def save_results(self, results, filename="articles"):
        """LÆ°u káº¿t quáº£ ra file"""
        # LÆ°u JSON
        json_file = f"{filename}.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        
        # LÆ°u CSV
        csv_file = f"{filename}.csv"
        if results:
            with open(csv_file, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=results[0].keys())
                writer.writeheader()
                writer.writerows(results)
        
        print(f"âœ… ÄÃ£ lÆ°u {len(results)} bÃ i viáº¿t vÃ o {json_file} vÃ  {csv_file}")

    def crawl_sample_urls(self):
        """Crawl cÃ¡c URL máº«u"""
        sample_urls = [
            "https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/",
            "https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html",
            "https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research"
        ]
        
        results = self.crawl_urls(sample_urls)
        if results:
            self.save_results(results, "sample_articles")
        
        return results


def main():
    """HÃ m chÃ­nh"""
    crawler = SimpleNewsCrawler()
    
    print("ğŸ“° Simple News Crawler - Test Version")
    print("=" * 40)
    print("1. Crawl URLs máº«u")
    print("2. Crawl URL tÃ¹y chá»‰nh")
    print("3. Crawl nhiá»u URLs tá»« file")
    
    choice = input("\nChá»n chá»©c nÄƒng (1-3): ").strip()
    
    if choice == "1":
        print("ğŸ” Crawling URLs máº«u...")
        results = crawler.crawl_sample_urls()
        print(f"\nâœ… HoÃ n thÃ nh! Thu tháº­p Ä‘Æ°á»£c {len(results)} bÃ i viáº¿t.")
        
    elif choice == "2":
        url = input("Nháº­p URL: ").strip()
        if url:
            use_ai = input("Sá»­ dá»¥ng AI tÃ³m táº¯t? (y/n, máº·c Ä‘á»‹nh y): ").strip().lower()
            use_summarizer = use_ai != 'n'
            
            result = crawler.process_article(url, use_summarizer)
            if result:
                crawler.save_results([result], "single_article")
                print("\nâœ… ÄÃ£ crawl thÃ nh cÃ´ng!")
            else:
                print("\nâŒ KhÃ´ng thá»ƒ crawl URL nÃ y!")
        else:
            print("âŒ URL khÃ´ng há»£p lá»‡!")
            
    elif choice == "3":
        filename = input("Nháº­p tÃªn file chá»©a URLs (má»—i dÃ²ng 1 URL): ").strip()
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
            
            if urls:
                use_ai = input("Sá»­ dá»¥ng AI tÃ³m táº¯t? (y/n, máº·c Ä‘á»‹nh y): ").strip().lower()
                use_summarizer = use_ai != 'n'
                
                results = crawler.crawl_urls(urls, use_summarizer)
                if results:
                    crawler.save_results(results, "batch_articles")
                print(f"\nâœ… HoÃ n thÃ nh! Thu tháº­p Ä‘Æ°á»£c {len(results)}/{len(urls)} bÃ i viáº¿t.")
            else:
                print("âŒ File rá»—ng hoáº·c khÃ´ng cÃ³ URL há»£p lá»‡!")
                
        except FileNotFoundError:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {filename}")
        except Exception as e:
            print(f"âŒ Lá»—i khi Ä‘á»c file: {e}")
            
    else:
        print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡!")


if __name__ == "__main__":
    main()
